{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fuR61udcHue",
        "outputId": "fe4060e6-5fe3-465e-8180-7b63174f4ce0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LXTaDXuyYJug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1baf31b5-44e8-44d7-b425-983165751dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MEMORY OPTIMIZATION SETTINGS\n",
            "============================================================\n",
            "Sample size: 150000\n",
            "Max TF-IDF features: 50000\n",
            "\n",
            "============================================================\n",
            "STEP 1: Loading Data\n",
            "============================================================\n",
            "Admissions data shape: (374139, 2)\n",
            "Loading discharge notes...\n",
            "Loading radiology notes...\n",
            "Discharge notes shape: (331731, 2)\n",
            "Radiology notes shape: (1144023, 2)\n",
            "\n",
            "============================================================\n",
            "STEP 2: Preparing Text Features (Memory Efficient)\n",
            "============================================================\n",
            "Combined notes shape: (1475754, 2)\n",
            "Aggregating notes by admission...\n",
            "Aggregated notes shape: (374139, 2)\n",
            "\n",
            "============================================================\n",
            "STEP 3: Preparing Final Dataset\n",
            "============================================================\n",
            "Final dataset shape: (374139, 3)\n",
            "\n",
            "‚ö†Ô∏è  Using stratified sample of 150000 records for memory efficiency\n",
            "Sampled dataset shape: (150000, 3)\n",
            "Readmission distribution:\n",
            "readmitted_30day\n",
            "0    75000\n",
            "1    75000\n",
            "Name: count, dtype: int64\n",
            "Readmission rate: 50.00%\n",
            "\n",
            "============================================================\n",
            "STEP 4: Defining Features and Target\n",
            "============================================================\n",
            "X shape: (150000,)\n",
            "y shape: (150000,)\n",
            "\n",
            "============================================================\n",
            "STEP 5: Train-Test Split\n",
            "============================================================\n",
            "Training set size: 120000\n",
            "Test set size: 30000\n",
            "Training readmission rate: 50.00%\n",
            "Test readmission rate: 50.00%\n",
            "\n",
            "============================================================\n",
            "STEP 6: TF-IDF Vectorization (Optimized)\n",
            "============================================================\n",
            "Fitting TF-IDF vectorizer on training data...\n",
            "Transforming test data...\n",
            "TF-IDF training matrix shape: (120000, 36420)\n",
            "TF-IDF test matrix shape: (30000, 36420)\n",
            "Number of features: 36420\n",
            "Matrix density: 1.1540%\n",
            "\n",
            "============================================================\n",
            "STEP 7: Training Logistic Regression Model (Optimized)\n",
            "============================================================\n",
            "Training model...\n",
            "convergence after 22 epochs took 32 seconds\n",
            "Model training complete!\n",
            "\n",
            "============================================================\n",
            "STEP 8: Model Evaluation\n",
            "============================================================\n",
            "Making predictions...\n",
            "\n",
            "üìä CLASSIFICATION REPORT:\n",
            "------------------------------------------------------------\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Not Readmitted       0.64      0.67      0.65     15000\n",
            "    Readmitted       0.65      0.63      0.64     15000\n",
            "\n",
            "      accuracy                           0.65     30000\n",
            "     macro avg       0.65      0.65      0.65     30000\n",
            "  weighted avg       0.65      0.65      0.65     30000\n",
            "\n",
            "\n",
            "üìà CONFUSION MATRIX:\n",
            "------------------------------------------------------------\n",
            "[[9999 5001]\n",
            " [5587 9413]]\n",
            "\n",
            "True Negatives:  9999\n",
            "False Positives: 5001\n",
            "False Negatives: 5587\n",
            "True Positives:  9413\n",
            "\n",
            "üéØ ROC-AUC SCORE: 0.7031\n",
            "\n",
            "üìå SUMMARY METRICS:\n",
            "------------------------------------------------------------\n",
            "Accuracy:  0.6471\n",
            "Precision: 0.6530\n",
            "Recall:    0.6275\n",
            "F1-Score:  0.6400\n",
            "ROC-AUC:   0.7031\n",
            "\n",
            "üîù TOP 15 MOST PREDICTIVE FEATURES FOR READMISSION:\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.5s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most predictive of READMISSION:\n",
            "  admissions: 4.2712\n",
            "  gastroparesis: 3.2191\n",
            "  discharged: 3.0377\n",
            "  ama: 2.9396\n",
            "  lymphoma: 2.8950\n",
            "  recently: 2.5270\n",
            "  scheduled: 2.4818\n",
            "  overdose: 2.4647\n",
            "  fix: 2.3801\n",
            "  aml: 2.3478\n",
            "  sarcoma: 2.3309\n",
            "  desensitization: 2.3307\n",
            "  dilaudid: 2.2801\n",
            "  recurrent: 2.2784\n",
            "  osteosarcoma: 2.2314\n",
            "\n",
            "Most predictive of NO READMISSION:\n",
            "  expired: -6.4279\n",
            "  hospice: -3.4069\n",
            "  cmo: -2.8911\n",
            "  comfort: -2.8721\n",
            "  adrs: -2.6578\n",
            "  uncomplicated: -2.3155\n",
            "  osh: -2.2041\n",
            "  file: -2.1036\n",
            "  detox: -1.9689\n",
            "  chronicity: -1.9287\n",
            "  adverse: -1.8991\n",
            "  hypoxemia: -1.8454\n",
            "  school: -1.8349\n",
            "  family: -1.8276\n",
            "  dni: -1.8263\n",
            "\n",
            "============================================================\n",
            "‚úÖ MODEL TRAINING AND EVALUATION COMPLETE!\n",
            "============================================================\n",
            "\n",
            "üí° TIPS TO USE FULL DATASET:\n",
            "------------------------------------------------------------\n",
            "1. Set USE_SAMPLING = False at the top\n",
            "2. Increase MAX_FEATURES gradually (2000 ‚Üí 3000 ‚Üí 5000)\n",
            "3. Use Colab Pro with more RAM\n",
            "4. Consider using SGDClassifier for even larger datasets\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# MEMORY OPTIMIZATION SETTINGS\n",
        "print(\"=\" * 60)\n",
        "print(\"MEMORY OPTIMIZATION SETTINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Reduce sample size for faster processing (adjust as needed)\n",
        "SAMPLE_SIZE = 150000  # Use subset of data\n",
        "USE_SAMPLING = True  # Set to False to use full dataset\n",
        "\n",
        "# Reduce TF-IDF features\n",
        "MAX_FEATURES = 50000  # Reduced from 5000\n",
        "MAX_DF = 0.90  # Filter very common terms\n",
        "MIN_DF = 10  # Filter very rare terms\n",
        "\n",
        "print(f\"Sample size: {SAMPLE_SIZE if USE_SAMPLING else 'Full dataset'}\")\n",
        "print(f\"Max TF-IDF features: {MAX_FEATURES}\")\n",
        "print()\n",
        "\n",
        "# 1. LOAD DATA WITH MEMORY OPTIMIZATION\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Loading Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/mimic_readmission_data\"\n",
        "\n",
        "# Load admissions with only needed columns\n",
        "df_admissions = pd.read_csv(\n",
        "    f\"{data_path}/admissions_with_readmission_labels.csv\",\n",
        "    usecols=['hadm_id', 'readmitted_30day']\n",
        ")\n",
        "\n",
        "print(f\"Admissions data shape: {df_admissions.shape}\")\n",
        "\n",
        "# Load notes in chunks and process efficiently\n",
        "print(\"Loading discharge notes...\")\n",
        "df_discharge = pd.read_csv(\n",
        "    f\"{data_path}/discharge_notes.csv\",\n",
        "    usecols=['hadm_id', 'text']\n",
        ")\n",
        "\n",
        "print(\"Loading radiology notes...\")\n",
        "df_radiology = pd.read_csv(\n",
        "    f\"{data_path}/radiology_notes.csv\",\n",
        "    usecols=['hadm_id', 'text']\n",
        ")\n",
        "\n",
        "print(f\"Discharge notes shape: {df_discharge.shape}\")\n",
        "print(f\"Radiology notes shape: {df_radiology.shape}\")\n",
        "print()\n",
        "\n",
        "# 2. PREPARE TEXT FEATURES - MEMORY EFFICIENT\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: Preparing Text Features (Memory Efficient)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Combine notes\n",
        "df_notes = pd.concat([df_discharge, df_radiology], ignore_index=True)\n",
        "print(f\"Combined notes shape: {df_notes.shape}\")\n",
        "\n",
        "# Clear memory\n",
        "del df_discharge, df_radiology\n",
        "gc.collect()\n",
        "\n",
        "# Group and aggregate with string limits to reduce memory\n",
        "print(\"Aggregating notes by admission...\")\n",
        "df_notes_agg = df_notes.groupby('hadm_id')['text'].apply(\n",
        "    lambda texts: ' '.join(str(text)[:5000] for text in texts if pd.notna(text))[:20000]  # Limit text length\n",
        ").reset_index()\n",
        "\n",
        "df_notes_agg.columns = ['hadm_id', 'all_notes_text']\n",
        "print(f\"Aggregated notes shape: {df_notes_agg.shape}\")\n",
        "\n",
        "# Clear memory\n",
        "del df_notes\n",
        "gc.collect()\n",
        "print()\n",
        "\n",
        "# 3. PREPARE FINAL DATASET\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 3: Preparing Final Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Merge with aggregated notes\n",
        "df_final = df_admissions.merge(df_notes_agg, on='hadm_id', how='inner')\n",
        "\n",
        "# Clear memory\n",
        "del df_admissions, df_notes_agg\n",
        "gc.collect()\n",
        "\n",
        "# Handle missing text data\n",
        "df_final['all_notes_text'] = df_final['all_notes_text'].fillna('')\n",
        "\n",
        "# Remove rows with empty text or missing target\n",
        "df_final = df_final[df_final['all_notes_text'].str.len() > 50]  # At least 50 chars\n",
        "df_final = df_final.dropna(subset=['readmitted_30day'])\n",
        "\n",
        "print(f\"Final dataset shape: {df_final.shape}\")\n",
        "\n",
        "# SAMPLING FOR MEMORY EFFICIENCY\n",
        "if USE_SAMPLING and len(df_final) > SAMPLE_SIZE:\n",
        "    print(f\"\\n‚ö†Ô∏è  Using stratified sample of {SAMPLE_SIZE} records for memory efficiency\")\n",
        "    df_final = df_final.groupby('readmitted_30day', group_keys=False).apply(\n",
        "        lambda x: x.sample(n=min(len(x), SAMPLE_SIZE // 2), random_state=42)\n",
        "    ).reset_index(drop=True)\n",
        "    print(f\"Sampled dataset shape: {df_final.shape}\")\n",
        "\n",
        "print(f\"Readmission distribution:\\n{df_final['readmitted_30day'].value_counts()}\")\n",
        "print(f\"Readmission rate: {df_final['readmitted_30day'].mean():.2%}\")\n",
        "print()\n",
        "\n",
        "# 4. DEFINE X AND y\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 4: Defining Features and Target\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "X = df_final['all_notes_text']\n",
        "y = df_final['readmitted_30day'].astype(int)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# Clear memory\n",
        "del df_final\n",
        "gc.collect()\n",
        "print()\n",
        "\n",
        "# 5. TRAIN-TEST SPLIT\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 5: Train-Test Split\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Clear memory\n",
        "del X, y\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Training readmission rate: {y_train.mean():.2%}\")\n",
        "print(f\"Test readmission rate: {y_test.mean():.2%}\")\n",
        "print()\n",
        "\n",
        "# 6. TF-IDF VECTORIZATION - OPTIMIZED\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 6: TF-IDF Vectorization (Optimized)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer with memory-efficient settings\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    max_features=MAX_FEATURES,\n",
        "    ngram_range=(1, 1),  # Only unigrams to save memory\n",
        "    min_df=MIN_DF,\n",
        "    max_df=MAX_DF,\n",
        "    strip_accents='unicode',\n",
        "    lowercase=True,\n",
        "    dtype=np.float32  # Use float32 instead of float64\n",
        ")\n",
        "\n",
        "# Fit and transform training data\n",
        "print(\"Fitting TF-IDF vectorizer on training data...\")\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "print(\"Transforming test data...\")\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF training matrix shape: {X_train_tfidf.shape}\")\n",
        "print(f\"TF-IDF test matrix shape: {X_test_tfidf.shape}\")\n",
        "print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
        "print(f\"Matrix density: {X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1]):.4%}\")\n",
        "\n",
        "# Clear memory\n",
        "del X_train, X_test\n",
        "gc.collect()\n",
        "print()\n",
        "\n",
        "# 7. TRAIN LOGISTIC REGRESSION - OPTIMIZED\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 7: Training Logistic Regression Model (Optimized)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize Logistic Regression with optimized settings\n",
        "logreg = LogisticRegression(\n",
        "    class_weight='balanced',\n",
        "    max_iter=500,  # Reduced iterations\n",
        "    random_state=42,\n",
        "    solver='saga',  # Better for large datasets\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    verbose=1  # Show progress\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training model...\")\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "print(\"Model training complete!\")\n",
        "print()\n",
        "\n",
        "# 8. EVALUATE\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 8: Model Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Make predictions\n",
        "print(\"Making predictions...\")\n",
        "y_pred = logreg.predict(X_test_tfidf)\n",
        "y_pred_proba = logreg.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nüìä CLASSIFICATION REPORT:\")\n",
        "print(\"-\" * 60)\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Readmitted', 'Readmitted']))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nüìà CONFUSION MATRIX:\")\n",
        "print(\"-\" * 60)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print()\n",
        "print(f\"True Negatives:  {cm[0, 0]}\")\n",
        "print(f\"False Positives: {cm[0, 1]}\")\n",
        "print(f\"False Negatives: {cm[1, 0]}\")\n",
        "print(f\"True Positives:  {cm[1, 1]}\")\n",
        "\n",
        "# ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"\\nüéØ ROC-AUC SCORE: {roc_auc:.4f}\")\n",
        "\n",
        "# Additional Metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nüìå SUMMARY METRICS:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "\n",
        "# Top predictive features\n",
        "print(\"\\nüîù TOP 15 MOST PREDICTIVE FEATURES FOR READMISSION:\")\n",
        "print(\"-\" * 60)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "coefficients = logreg.coef_[0]\n",
        "\n",
        "# Get indices of top positive coefficients\n",
        "top_positive_idx = np.argsort(coefficients)[-15:][::-1]\n",
        "print(\"\\nMost predictive of READMISSION:\")\n",
        "for idx in top_positive_idx:\n",
        "    print(f\"  {feature_names[idx]}: {coefficients[idx]:.4f}\")\n",
        "\n",
        "# Get indices of top negative coefficients\n",
        "top_negative_idx = np.argsort(coefficients)[:15]\n",
        "print(\"\\nMost predictive of NO READMISSION:\")\n",
        "for idx in top_negative_idx:\n",
        "    print(f\"  {feature_names[idx]}: {coefficients[idx]:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ MODEL TRAINING AND EVALUATION COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nüí° TIPS TO USE FULL DATASET:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"1. Set USE_SAMPLING = False at the top\")\n",
        "print(\"2. Increase MAX_FEATURES gradually (2000 ‚Üí 3000 ‚Üí 5000)\")\n",
        "print(\"3. Use Colab Pro with more RAM\")\n",
        "print(\"4. Consider using SGDClassifier for even larger datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQEPqdjBkQMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}