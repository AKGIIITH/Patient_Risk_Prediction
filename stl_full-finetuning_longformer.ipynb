{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13769531,"sourceType":"datasetVersion","datasetId":8763276}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:06:17.049751Z","iopub.execute_input":"2025-11-18T08:06:17.050098Z","iopub.status.idle":"2025-11-18T08:06:17.054302Z","shell.execute_reply.started":"2025-11-18T08:06:17.050073Z","shell.execute_reply":"2025-11-18T08:06:17.053604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    Trainer,\n    TrainingArguments,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score\n)\nfrom huggingface_hub import login, HfApi\nimport warnings\nimport logging\nimport os\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nos.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n\nprint(\"âœ“ All imports successful\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:06:17.055511Z","iopub.execute_input":"2025-11-18T08:06:17.055994Z","iopub.status.idle":"2025-11-18T08:06:50.477002Z","shell.execute_reply.started":"2025-11-18T08:06:17.055976Z","shell.execute_reply":"2025-11-18T08:06:50.476183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# HUGGING FACE CONFIGURATION\n# ============================================================================\n\nclass HF_CONFIG:\n    \"\"\"Hugging Face Hub configuration\"\"\"\n    \n    # Your Hugging Face username and model repository name\n    HUB_USERNAME = \"AKG2\"  # â† Change this to your HF username\n    HUB_MODEL_NAME = \"clinical-longformer-readmission-stl\"  # â† Change model name if desired\n    \n    # This will be constructed as: username/model-name\n    @property\n    def HUB_MODEL_ID(self):\n        return f\"{self.HUB_USERNAME}/{self.HUB_MODEL_NAME}\"\n    \n    # Kaggle secret name for HF token (don't change unless you named it differently)\n    KAGGLE_SECRET_NAME = \"HF_TOKEN\"\n    \n    # Whether to push to hub during training\n    PUSH_TO_HUB = True\n    \n    # Whether to make the repository private\n    PRIVATE_REPO = False\n\n\n# ============================================================================\n# IMPROVED TRAINING CONFIGURATION\n# ============================================================================\n\nclass CONFIG:\n    \"\"\"Training configuration with improvements\"\"\"\n    \n    # Data paths\n    DATA_PATH = \"/kaggle/input/mimic-iv\"\n    MODEL_NAME = \"yikuan8/Clinical-Longformer\"  # Keep same base model\n    \n    # Hugging Face configuration\n    HF = HF_CONFIG()\n    \n    # Dataset settings\n    SAMPLE_SIZE = 10000  # Use full dataset for better training\n    TEST_SIZE = 0.2\n    RANDOM_SEED = 42\n    \n    # CRITICAL: Balance the dataset\n    BALANCE_CLASSES = True  # NEW: Undersample majority class\n    MAX_SAMPLES_PER_CLASS = 75000  # NEW: Limit to balance with minority\n    \n    # Model settings\n    MAX_LENGTH = 2048  # CHANGED: Reduced from 4096 for better memory/speed\n    \n    # Training hyperparameters - MAJOR CHANGES\n    EPOCHS = 10  # CHANGED: More epochs with early stopping\n    TRAIN_BATCH_SIZE = 4  # CHANGED: Increased from 2\n    VALID_BATCH_SIZE = 8  # CHANGED: Increased from 4\n    GRADIENT_ACCUMULATION = 8  # CHANGED: Reduced from 16\n    LEARNING_RATE = 5e-5  # CHANGED: Increased from 2e-5 for faster convergence\n    WEIGHT_DECAY = 0.01\n    WARMUP_RATIO = 0.05  # CHANGED: Reduced from 0.1 for faster warmup\n    MAX_GRAD_NORM = 1.0  # NEW: Clip gradients to prevent instability\n    \n    # Optimizer settings\n    ADAM_EPSILON = 1e-8  # NEW: For numerical stability\n    ADAM_BETA1 = 0.9  # NEW\n    ADAM_BETA2 = 0.999  # NEW\n    \n    # Early stopping\n    EARLY_STOPPING_PATIENCE = 3  # NEW: Stop if no improvement\n    \n    # GPU optimization\n    USE_FP16 = True\n    \n    # Output directory\n    OUTPUT_DIR = \"./stl_readmission_model_improved\"\n\n# ============================================================================\n# HUGGING FACE HUB LOGIN\n# ============================================================================\n\ndef login_to_huggingface(config):\n    \"\"\"Log in to Hugging Face Hub using Kaggle secret\"\"\"\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        hf_token = user_secrets.get_secret(config.HF.KAGGLE_SECRET_NAME)\n        \n        if hf_token:\n            login(token=hf_token, add_to_git_credential=True)\n            print(f\"âœ“ Successfully logged in to Hugging Face Hub\")\n            print(f\"  Model will be pushed to: {config.HF.HUB_MODEL_ID}\")\n            return True\n        else:\n            print(\"âš  HF_TOKEN not found in Kaggle secrets\")\n            return False\n    except Exception as e:\n        print(f\"âš  Could not log in to Hugging Face Hub: {e}\")\n        print(\"  Training will continue without Hub integration\")\n        return False\n\n\n# ============================================================================\n# IMPROVED DATA LOADING WITH CLASS BALANCING\n# ============================================================================\n\ndef load_and_prepare_data(config):\n    \"\"\"Load and merge all data sources with class balancing\"\"\"\n    print(\"=\" * 80)\n    print(\"LOADING DATA\")\n    print(\"=\" * 80)\n    \n    # Load main admissions file\n    admissions = pd.read_csv(f\"{config.DATA_PATH}/admissions_with_readmission_labels.csv\")\n    print(f\"âœ“ Loaded admissions: {admissions.shape}\")\n    \n    # Load discharge notes\n    discharge = pd.read_csv(f\"{config.DATA_PATH}/discharge_notes-001.csv\")\n    print(f\"âœ“ Loaded discharge notes: {discharge.shape}\")\n    \n    # Load radiology notes\n    radiology = pd.read_csv(f\"{config.DATA_PATH}/radiology_notes.csv\")\n    print(f\"âœ“ Loaded radiology notes: {radiology.shape}\")\n    \n    # Combine notes\n    print(\"\\nCombining notes...\")\n    discharge_grouped = discharge.groupby('hadm_id')['text'].apply(\n        lambda x: ' '.join(x.astype(str))\n    ).reset_index()\n    discharge_grouped.columns = ['hadm_id', 'discharge_text']\n    \n    radiology_grouped = radiology.groupby('hadm_id')['text'].apply(\n        lambda x: ' '.join(x.astype(str))\n    ).reset_index()\n    radiology_grouped.columns = ['hadm_id', 'radiology_text']\n    \n    # Merge notes\n    notes_combined = discharge_grouped.merge(\n        radiology_grouped, on='hadm_id', how='outer'\n    )\n    \n    # Combine all text\n    notes_combined['combined_text'] = (\n        notes_combined['discharge_text'].fillna('') + ' ' + \n        notes_combined['radiology_text'].fillna('')\n    )\n    notes_combined['combined_text'] = notes_combined['combined_text'].str.strip()\n    \n    # Merge with admissions\n    df = admissions.merge(notes_combined[['hadm_id', 'combined_text']], \n                          on='hadm_id', how='left')\n    \n    # Use combined_text if available\n    df['final_text'] = df['combined_text'].fillna(df.get('text', ''))\n    df['final_text'] = df['final_text'].fillna('')\n    \n    # Remove rows with empty text\n    df = df[df['final_text'].str.len() > 0]\n    print(f\"âœ“ Final merged data: {df.shape}\")\n    \n    # Prepare readmission labels\n    print(\"\\nPreparing labels...\")\n    df['readmitted_30day'] = df['readmitted_30day'].astype(int)\n    \n    print(f\"  - Original distribution: {df['readmitted_30day'].value_counts().to_dict()}\")\n    \n    # ========================================================================\n    # CRITICAL: BALANCE THE DATASET\n    # ========================================================================\n    if config.BALANCE_CLASSES:\n        print(\"\\nâš–ï¸  Balancing dataset...\")\n        \n        # Separate positive and negative samples\n        pos_samples = df[df['readmitted_30day'] == 1]\n        neg_samples = df[df['readmitted_30day'] == 0]\n        \n        print(f\"  - Positive samples: {len(pos_samples)}\")\n        print(f\"  - Negative samples: {len(neg_samples)}\")\n        \n        # Determine target size for each class\n        target_size = min(\n            len(pos_samples),\n            len(neg_samples),\n            config.MAX_SAMPLES_PER_CLASS\n        )\n        \n        print(f\"  - Target size per class: {target_size}\")\n        \n        # Sample both classes to target size\n        pos_samples = pos_samples.sample(n=target_size, random_state=config.RANDOM_SEED)\n        neg_samples = neg_samples.sample(n=target_size, random_state=config.RANDOM_SEED)\n        \n        # Combine and shuffle\n        df = pd.concat([pos_samples, neg_samples], ignore_index=True)\n        df = df.sample(frac=1, random_state=config.RANDOM_SEED).reset_index(drop=True)\n        \n        print(f\"  - Balanced dataset size: {len(df)}\")\n        print(f\"  - New distribution: {df['readmitted_30day'].value_counts().to_dict()}\")\n    \n    # Sample data if needed (for testing)\n    if config.SAMPLE_SIZE is not None:\n        df = df.sample(n=min(config.SAMPLE_SIZE, len(df)), \n                       random_state=config.RANDOM_SEED)\n        print(f\"âœ“ Sampled {len(df)} rows for testing\")\n    \n    # Calculate class weights (should be 1.0 if balanced)\n    pos_count = df['readmitted_30day'].sum()\n    neg_count = len(df) - pos_count\n    pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n    \n    print(f\"\\n  - Final positive weight: {pos_weight:.2f}\")\n    print(f\"  - Final dataset size: {len(df)}\")\n    \n    return df, pos_weight\n\n# ============================================================================\n# DATASET CLASS\n# ============================================================================\n\nclass ReadmissionDataset(Dataset):\n    \"\"\"Dataset for single-task readmission prediction\"\"\"\n    \n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        \n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n        }\n\n\n# ============================================================================\n# IMPROVED SINGLE-TASK MODEL\n# ============================================================================\n\nclass ReadmissionModel(nn.Module):\n    \"\"\"Improved single-task model for 30-day readmission prediction\"\"\"\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        # Backbone: Clinical-Longformer\n        self.backbone = AutoModel.from_pretrained(model_name)\n        hidden_size = self.backbone.config.hidden_size\n        \n        # IMPROVED: Better classification head with layer normalization\n        self.dropout1 = nn.Dropout(0.1)\n        self.layer_norm = nn.LayerNorm(hidden_size)  # NEW: Stabilizes training\n        \n        # NEW: Add intermediate layer for better representation\n        self.intermediate = nn.Linear(hidden_size, hidden_size // 2)\n        self.activation = nn.GELU()  # NEW: Better than ReLU for transformers\n        self.dropout2 = nn.Dropout(0.2)\n        \n        # Final classifier\n        self.classifier = nn.Linear(hidden_size // 2, 1)\n        \n        # Initialize weights properly\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights for better convergence\"\"\"\n        nn.init.xavier_uniform_(self.intermediate.weight)\n        nn.init.zeros_(self.intermediate.bias)\n        nn.init.xavier_uniform_(self.classifier.weight)\n        nn.init.zeros_(self.classifier.bias)\n    \n    def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n        # Get [CLS] token representation from backbone\n        outputs = self.backbone(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Use [CLS] token (first token)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        \n        # Apply improvements\n        cls_output = self.dropout1(cls_output)\n        cls_output = self.layer_norm(cls_output)  # Stabilize\n        \n        # Intermediate layer\n        hidden = self.intermediate(cls_output)\n        hidden = self.activation(hidden)\n        hidden = self.dropout2(hidden)\n        \n        # Final prediction\n        logits = self.classifier(hidden)\n        \n        return {'logits': logits}\n\n# ============================================================================\n# IMPROVED CUSTOM TRAINER\n# ============================================================================\n\nclass ImprovedWeightedLossTrainer(Trainer):\n    \"\"\"Custom trainer with improvements for stability and convergence\"\"\"\n    \n    def __init__(self, pos_weight, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pos_weight = torch.tensor([pos_weight])\n        self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n        \n        # Track training metrics\n        self.train_losses = []\n        self.eval_metrics = []\n    \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        # Move pos_weight to correct device\n        model_device = next(model.parameters()).device\n        if self.pos_weight.device != model_device:\n            self.pos_weight = self.pos_weight.to(model_device)\n            self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n        \n        # Extract labels\n        labels = inputs.pop('labels')\n        \n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs['logits'].squeeze()\n        \n        # Calculate loss\n        loss = self.bce_loss(logits, labels)\n        \n        # IMPROVEMENT: Add L2 regularization on logits to prevent extreme values\n        l2_reg = 0.001 * torch.mean(logits ** 2)\n        loss = loss + l2_reg\n        \n        # Track losses\n        self.train_losses.append(loss.item())\n        \n        return (loss, outputs) if return_outputs else loss\n    \n    def log(self, logs, start_time=None):\n        \"\"\"Enhanced logging\"\"\"\n        super().log(logs, start_time=start_time)\n        \n        # Print cleaner progress updates\n        if 'loss' in logs:\n            epoch = logs.get('epoch', 0)\n            loss = logs.get('loss', 0)\n            lr = logs.get('learning_rate', 0)\n            grad_norm = logs.get('grad_norm', 0)\n            \n            print(f\"Epoch {epoch:.2f} | Loss: {loss:.4f} | LR: {lr:.2e} | Grad: {grad_norm:.2f}\")\n        \n        if 'eval_roc_auc' in logs:\n            print(f\"\\nðŸ“Š Evaluation Results:\")\n            print(f\"   ROC-AUC:   {logs['eval_roc_auc']:.4f}\")\n            print(f\"   Accuracy:  {logs['eval_accuracy']:.4f}\")\n            print(f\"   Precision: {logs['eval_precision']:.4f}\")\n            print(f\"   Recall:    {logs['eval_recall']:.4f}\")\n            print(f\"   F1 Score:  {logs['eval_f1']:.4f}\")\n            print(f\"   Loss:      {logs['eval_loss']:.4f}\\n\")\n            \n            self.eval_metrics.append(logs)\n\n\n# ============================================================================\n# EARLY STOPPING CALLBACK\n# ============================================================================\n\nfrom transformers import TrainerCallback, TrainerState, TrainerControl\n\nclass EarlyStoppingCallback(TrainerCallback):\n    \"\"\"Early stopping callback to prevent overfitting\"\"\"\n    \n    def __init__(self, patience=3, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_metric = None\n        self.wait = 0\n        self.stopped_epoch = 0\n    \n    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n        metric_value = metrics.get(\"eval_roc_auc\")\n        \n        if metric_value is None:\n            return control\n        \n        if self.best_metric is None:\n            self.best_metric = metric_value\n        elif metric_value > self.best_metric + self.min_delta:\n            self.best_metric = metric_value\n            self.wait = 0\n            print(f\"âœ“ New best ROC-AUC: {metric_value:.4f}\")\n        else:\n            self.wait += 1\n            print(f\"âš ï¸  No improvement for {self.wait} epoch(s)\")\n            \n            if self.wait >= self.patience:\n                self.stopped_epoch = state.epoch\n                control.should_training_stop = True\n                print(f\"\\nðŸ›‘ Early stopping triggered after {self.wait} epochs without improvement\")\n        \n        return control\n        \n# ============================================================================\n# EVALUATION METRICS\n# ============================================================================\n\ndef compute_metrics(eval_pred):\n    \"\"\"Compute metrics for readmission prediction\"\"\"\n    predictions, labels = eval_pred\n    \n    # Extract logits (handle both dict and tensor formats)\n    if isinstance(predictions, dict):\n        logits = predictions['logits']\n    elif isinstance(predictions, tuple):\n        logits = predictions[0]\n    else:\n        logits = predictions\n    \n    # Convert to numpy if needed\n    logits_np = logits.numpy() if isinstance(logits, torch.Tensor) else logits\n    \n    # Convert logits to probabilities (sigmoid)\n    probs = 1 / (1 + np.exp(-logits_np.squeeze()))\n    preds = (probs > 0.5).astype(int)\n    \n    # Handle edge case: if all predictions are same class\n    try:\n        roc_auc = roc_auc_score(labels, probs)\n    except ValueError:\n        roc_auc = 0.0\n    \n    # Calculate metrics\n    metrics = {\n        'roc_auc': roc_auc,\n        'accuracy': accuracy_score(labels, preds),\n        'precision': precision_score(labels, preds, zero_division=0),\n        'recall': recall_score(labels, preds, zero_division=0),\n        'f1': f1_score(labels, preds, zero_division=0)\n    }\n    \n    return metrics\n\n\n# ============================================================================\n# CHECK FOR EXISTING MODEL ON HUB\n# ============================================================================\n\ndef check_hub_model_exists(config):\n    \"\"\"Check if model already exists on Hugging Face Hub\"\"\"\n    try:\n        api = HfApi()\n        repos = api.list_models(author=config.HF.HUB_USERNAME)\n        model_exists = any(repo.modelId == config.HF.HUB_MODEL_ID for repo in repos)\n        \n        if model_exists:\n            print(f\"âœ“ Found existing model on Hub: {config.HF.HUB_MODEL_ID}\")\n            return True\n        else:\n            print(f\"  No existing model found on Hub, will create new repository\")\n            return False\n    except Exception as e:\n        print(f\"  Could not check Hub for existing model: {e}\")\n        return False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:13:10.656883Z","iopub.execute_input":"2025-11-18T08:13:10.657665Z","iopub.status.idle":"2025-11-18T08:13:10.691106Z","shell.execute_reply.started":"2025-11-18T08:13:10.657629Z","shell.execute_reply":"2025-11-18T08:13:10.690353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"SINGLE-TASK LEARNING FOR HOSPITAL READMISSION PREDICTION\")\nprint(\"=\" * 80 + \"\\n\")\n\n# Set random seeds\ntorch.manual_seed(CONFIG.RANDOM_SEED)\nnp.random.seed(CONFIG.RANDOM_SEED)\n\n# Login to Hugging Face Hub\nhf_logged_in = login_to_huggingface(CONFIG)\n\n# Check if model exists on Hub\nmodel_exists_on_hub = False\nif hf_logged_in and CONFIG.HF.PUSH_TO_HUB:\n    model_exists_on_hub = check_hub_model_exists(CONFIG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:06:50.512579Z","iopub.execute_input":"2025-11-18T08:06:50.512825Z","iopub.status.idle":"2025-11-18T08:06:51.280989Z","shell.execute_reply.started":"2025-11-18T08:06:50.512807Z","shell.execute_reply":"2025-11-18T08:06:51.280190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ndf, pos_weight = load_and_prepare_data(CONFIG)\n\n# Split data\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SPLITTING DATA\")\nprint(\"=\" * 80)\ntrain_df, val_df = train_test_split(\n    df, \n    test_size=CONFIG.TEST_SIZE, \n    random_state=CONFIG.RANDOM_SEED,\n    stratify=df['readmitted_30day']\n)\nprint(f\"âœ“ Train size: {len(train_df)}\")\nprint(f\"âœ“ Validation size: {len(val_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:06:51.282579Z","iopub.execute_input":"2025-11-18T08:06:51.282931Z","iopub.status.idle":"2025-11-18T08:09:09.425137Z","shell.execute_reply.started":"2025-11-18T08:06:51.282913Z","shell.execute_reply":"2025-11-18T08:09:09.424296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load tokenizer\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LOADING TOKENIZER\")\nprint(\"=\" * 80)\n\n# Try to load from Hub first if model exists there\ntokenizer_source = CONFIG.MODEL_NAME\nif model_exists_on_hub:\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(CONFIG.HF.HUB_MODEL_ID)\n        tokenizer_source = CONFIG.HF.HUB_MODEL_ID\n        print(f\"âœ“ Loaded tokenizer from Hub: {tokenizer_source}\")\n    except:\n        tokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_NAME)\n        print(f\"âœ“ Loaded tokenizer from base model: {tokenizer_source}\")\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_NAME)\n    print(f\"âœ“ Loaded tokenizer: {tokenizer_source}\")\n\n# Create datasets\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CREATING DATASETS\")\nprint(\"=\" * 80)\ntrain_dataset = ReadmissionDataset(\n    texts=train_df['final_text'].values,\n    labels=train_df['readmitted_30day'].values,\n    tokenizer=tokenizer,\n    max_length=CONFIG.MAX_LENGTH\n)\n\nval_dataset = ReadmissionDataset(\n    texts=val_df['final_text'].values,\n    labels=val_df['readmitted_30day'].values,\n    tokenizer=tokenizer,\n    max_length=CONFIG.MAX_LENGTH\n)\nprint(f\"âœ“ Train dataset: {len(train_dataset)} samples\")\nprint(f\"âœ“ Validation dataset: {len(val_dataset)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:09:09.426013Z","iopub.execute_input":"2025-11-18T08:09:09.426294Z","iopub.status.idle":"2025-11-18T08:09:16.293208Z","shell.execute_reply.started":"2025-11-18T08:09:09.426267Z","shell.execute_reply":"2025-11-18T08:09:16.292539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# IMPROVED TRAINING SETUP AND EXECUTION\n# ============================================================================\n\n# Initialize model\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INITIALIZING IMPROVED MODEL\")\nprint(\"=\" * 80)\n\nmodel = ReadmissionModel(CONFIG.MODEL_NAME)\nprint(f\"âœ“ Initialized model from: {CONFIG.MODEL_NAME}\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"  - Total parameters: {total_params:,}\")\nprint(f\"  - Trainable parameters: {trainable_params:,}\")\n\n# IMPROVED: Training arguments with better hyperparameters\ntraining_args = TrainingArguments(\n    output_dir=CONFIG.OUTPUT_DIR,\n    num_train_epochs=CONFIG.EPOCHS,\n    per_device_train_batch_size=CONFIG.TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=CONFIG.VALID_BATCH_SIZE,\n    gradient_accumulation_steps=CONFIG.GRADIENT_ACCUMULATION,\n    \n    # Learning rate and optimization\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    warmup_ratio=CONFIG.WARMUP_RATIO,\n    max_grad_norm=CONFIG.MAX_GRAD_NORM,  # NEW: Gradient clipping\n    \n    # Optimizer settings\n    adam_epsilon=CONFIG.ADAM_EPSILON,  # NEW\n    adam_beta1=CONFIG.ADAM_BETA1,  # NEW\n    adam_beta2=CONFIG.ADAM_BETA2,  # NEW\n    \n    # Mixed precision\n    fp16=CONFIG.USE_FP16,\n    \n    # Logging and evaluation\n    logging_steps=50,  # CHANGED: Less frequent logging\n    logging_first_step=True,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    \n    # Model selection\n    load_best_model_at_end=True,\n    metric_for_best_model=\"roc_auc\",\n    greater_is_better=True,\n    save_total_limit=3,  # Keep top 3 checkpoints\n    \n    # Other settings\n    report_to=\"none\",\n    seed=CONFIG.RANDOM_SEED,\n    disable_tqdm=True,\n    \n    # Hugging Face Hub\n    push_to_hub=CONFIG.HF.PUSH_TO_HUB and hf_logged_in,\n    hub_model_id=CONFIG.HF.HUB_MODEL_ID if hf_logged_in else None,\n    hub_strategy=\"end\",  # CHANGED: Only push at end\n    hub_private_repo=CONFIG.HF.PRIVATE_REPO,\n)\n\n# Initialize callbacks\ncallbacks = [\n    EarlyStoppingCallback(\n        patience=CONFIG.EARLY_STOPPING_PATIENCE,\n        min_delta=0.001\n    )\n]\n\n# Initialize trainer\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INITIALIZING IMPROVED TRAINER\")\nprint(\"=\" * 80)\n\ntrainer = ImprovedWeightedLossTrainer(\n    pos_weight=pos_weight,\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=callbacks,\n)\n\nprint(\"âœ“ Trainer initialized with improvements:\")\nprint(f\"  - Effective batch size: {CONFIG.TRAIN_BATCH_SIZE * CONFIG.GRADIENT_ACCUMULATION}\")\nprint(f\"  - Learning rate: {CONFIG.LEARNING_RATE}\")\nprint(f\"  - Gradient clipping: {CONFIG.MAX_GRAD_NORM}\")\nprint(f\"  - Early stopping patience: {CONFIG.EARLY_STOPPING_PATIENCE}\")\nprint(f\"  - Max length: {CONFIG.MAX_LENGTH}\")\n\n# Check for existing checkpoints\ncheckpoint_path = None\nif os.path.exists(CONFIG.OUTPUT_DIR):\n    checkpoints = [\n        os.path.join(CONFIG.OUTPUT_DIR, d) \n        for d in os.listdir(CONFIG.OUTPUT_DIR) \n        if d.startswith(\"checkpoint-\")\n    ]\n    if checkpoints:\n        checkpoint_path = max(checkpoints, key=os.path.getmtime)\n        print(f\"\\nâœ“ Found existing checkpoint: {checkpoint_path}\")\n        print(\"  Training will resume from this checkpoint\")\n\n# Train\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING MODEL WITH IMPROVEMENTS\")\nprint(\"=\" * 80)\nprint(\"Expected improvements:\")\nprint(\"  âœ“ Balanced dataset (equal class distribution)\")\nprint(\"  âœ“ Higher learning rate for faster convergence\")\nprint(\"  âœ“ Gradient clipping to prevent instability\")\nprint(\"  âœ“ Better model architecture with intermediate layer\")\nprint(\"  âœ“ Early stopping to prevent overfitting\")\nprint(\"=\" * 80 + \"\\n\")\n\nif checkpoint_path:\n    trainer.train(resume_from_checkpoint=checkpoint_path)\nelse:\n    trainer.train()\n\n# Print training summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\" * 80)\n\nif hasattr(trainer, 'train_losses') and len(trainer.train_losses) > 0:\n    avg_train_loss = sum(trainer.train_losses) / len(trainer.train_losses)\n    print(f\"Average training loss: {avg_train_loss:.4f}\")\n    print(f\"Final training loss: {trainer.train_losses[-1]:.4f}\")\n\nif hasattr(trainer, 'eval_metrics') and len(trainer.eval_metrics) > 0:\n    best_eval = max(trainer.eval_metrics, key=lambda x: x.get('eval_roc_auc', 0))\n    print(f\"\\nBest evaluation metrics:\")\n    print(f\"  ROC-AUC:   {best_eval['eval_roc_auc']:.4f}\")\n    print(f\"  Accuracy:  {best_eval['eval_accuracy']:.4f}\")\n    print(f\"  F1 Score:  {best_eval['eval_f1']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:13:15.908028Z","iopub.execute_input":"2025-11-18T08:13:15.908737Z","iopub.status.idle":"2025-11-18T08:13:52.786342Z","shell.execute_reply.started":"2025-11-18T08:13:15.908712Z","shell.execute_reply":"2025-11-18T08:13:52.785566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final evaluation\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINAL EVALUATION ON BEST MODEL\")\nprint(\"=\" * 80)\n\neval_results = trainer.evaluate()\n\nprint(\"\\nðŸ“Š FINAL RESULTS:\")\nprint(f\"  ROC-AUC:   {eval_results['eval_roc_auc']:.4f}\")\nprint(f\"  Accuracy:  {eval_results['eval_accuracy']:.4f}\")\nprint(f\"  Precision: {eval_results['eval_precision']:.4f}\")\nprint(f\"  Recall:    {eval_results['eval_recall']:.4f}\")\nprint(f\"  F1 Score:  {eval_results['eval_f1']:.4f}\")\nprint(f\"  Loss:      {eval_results['eval_loss']:.4f}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:16:04.984498Z","iopub.execute_input":"2025-11-18T08:16:04.985053Z","iopub.status.idle":"2025-11-18T08:16:07.829736Z","shell.execute_reply.started":"2025-11-18T08:16:04.985028Z","shell.execute_reply":"2025-11-18T08:16:07.829024Z"}},"outputs":[],"execution_count":null}]}