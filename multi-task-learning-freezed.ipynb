{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13769531,"sourceType":"datasetVersion","datasetId":8763276}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install/upgrade transformers and accelerate (for the Trainer)\n!pip install transformers --upgrade --quiet\n!pip install accelerate --upgrade --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:02:43.920557Z","iopub.execute_input":"2025-11-17T21:02:43.920893Z","iopub.status.idle":"2025-11-17T21:02:43.925758Z","shell.execute_reply.started":"2025-11-17T21:02:43.920845Z","shell.execute_reply":"2025-11-17T21:02:43.925104Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nMulti-Task Learning Classifier for Hospital Readmission Prediction\nUses Clinical-Longformer with three task heads:\n1. Main: 30-day readmission (binary)\n2. Auxiliary: Hospital mortality (binary)\n3. Auxiliary: Admission type (multiclass)\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score\n)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# ===========================================================================\n# CONFIGURATION\n# ===========================================================================\nclass CONFIG:\n    DATA_PATH = \"/kaggle/input/mimic-iv\"\n    MODEL_NAME = \"yikuan8/Clinical-Longformer\"\n    \n    # --- Fast Testing Configs ---\n    SAMPLE_SIZE = 10000  # Set to None to use full dataset\n    EPOCHS = 1\n    \n    # --- GPU Optimization ---\n    TRAIN_BATCH_SIZE = 2\n    VALID_BATCH_SIZE = 4\n    GRADIENT_ACCUMULATION = 16\n    USE_FP16 = True\n    MAX_LENGTH = 4096\n    \n    # --- Training Hyperparameters ---\n    LEARNING_RATE = 2e-5\n    WEIGHT_DECAY = 0.01\n    WARMUP_RATIO = 0.1\n    \n    # --- Loss Weights ---\n    WEIGHT_READMIT = 1.0\n    WEIGHT_MORTALITY = 0.5\n    WEIGHT_ADM_TYPE = 0.5\n    \n    # --- Other ---\n    RANDOM_SEED = 42\n    TEST_SIZE = 0.2\n    OUTPUT_DIR = \"./mtl_readmission_model_FROZEN\" # Changed output dir\n\n\n# ===========================================================================\n# DATA LOADING AND PREPARATION\n# ===========================================================================\ndef load_and_prepare_data(config):\n    \"\"\"Load and merge all data sources.\"\"\"\n    print(\"=\" * 80)\n    print(\"LOADING DATA\")\n    print(\"=\" * 80)\n    \n    # Load main admissions file\n    admissions = pd.read_csv(f\"{config.DATA_PATH}/admissions_with_readmission_labels.csv\")\n    print(f\"‚úì Loaded admissions: {admissions.shape}\")\n    \n    # Load discharge notes\n    discharge = pd.read_csv(f\"{config.DATA_PATH}/discharge_notes-001.csv\")\n    print(f\"‚úì Loaded discharge notes: {discharge.shape}\")\n    \n    # Load radiology notes\n    radiology = pd.read_csv(f\"{config.DATA_PATH}/radiology_notes.csv\")\n    print(f\"‚úì Loaded radiology notes: {radiology.shape}\")\n    \n    # Combine notes: concatenate discharge and radiology by hadm_id\n    print(\"\\nCombining notes...\")\n    discharge_grouped = discharge.groupby('hadm_id')['text'].apply(\n        lambda x: ' '.join(x.astype(str))\n    ).reset_index()\n    discharge_grouped.columns = ['hadm_id', 'discharge_text']\n    \n    radiology_grouped = radiology.groupby('hadm_id')['text'].apply(\n        lambda x: ' '.join(x.astype(str))\n    ).reset_index()\n    radiology_grouped.columns = ['hadm_id', 'radiology_text']\n    \n    # Merge notes\n    notes_combined = discharge_grouped.merge(\n        radiology_grouped, on='hadm_id', how='outer'\n    )\n    \n    # Combine all text\n    notes_combined['combined_text'] = (\n        notes_combined['discharge_text'].fillna('') + ' ' + \n        notes_combined['radiology_text'].fillna('')\n    )\n    notes_combined['combined_text'] = notes_combined['combined_text'].str.strip()\n    \n    # Merge with admissions\n    df = admissions.merge(notes_combined[['hadm_id', 'combined_text']], \n                          on='hadm_id', how='left')\n    \n    # Use combined_text if available, otherwise use original text\n    df['final_text'] = df['combined_text'].fillna(df.get('text', ''))\n    df['final_text'] = df['final_text'].fillna('')\n    \n    print(f\"‚úì Final merged data: {df.shape}\")\n    \n    # Sample data if needed\n    if config.SAMPLE_SIZE is not None:\n        df = df.sample(n=min(config.SAMPLE_SIZE, len(df)), \n                       random_state=config.RANDOM_SEED)\n        print(f\"‚úì Sampled {len(df)} rows for testing\")\n    \n    # Prepare labels\n    print(\"\\nPreparing labels...\")\n    \n    # Binary labels\n    df['readmitted_30day'] = df['readmitted_30day'].astype(int)\n    df['hospital_expire_flag'] = df['hospital_expire_flag'].astype(int)\n    \n    # Multiclass label (admission_type)\n    le = LabelEncoder()\n    df['admission_type_encoded'] = le.fit_transform(df['admission_type'])\n    num_admission_types = len(le.classes_)\n    \n    print(f\"  - Readmission distribution: {df['readmitted_30day'].value_counts().to_dict()}\")\n    print(f\"  - Mortality distribution: {df['hospital_expire_flag'].value_counts().to_dict()}\")\n    print(f\"  - Admission types: {num_admission_types} classes\")\n    \n    # Calculate class weights for readmission (handle imbalance)\n    pos_count = df['readmitted_30day'].sum()\n    neg_count = len(df) - pos_count\n    pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n    \n    print(f\"  - Positive weight for readmission: {pos_weight:.2f}\")\n    \n    return df, le, num_admission_types, pos_weight\n\n\n# ===========================================================================\n# DATASET CLASS\n# ===========================================================================\nclass MTLDataset(Dataset):\n    \"\"\"Custom dataset for multi-task learning.\"\"\"\n    \n    def __init__(self, texts, readmit_labels, mortality_labels, \n                 adm_type_labels, tokenizer, max_length):\n        self.texts = texts\n        self.readmit_labels = readmit_labels\n        self.mortality_labels = mortality_labels\n        self.adm_type_labels = adm_type_labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        \n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'readmit_label': torch.tensor(self.readmit_labels[idx], dtype=torch.float),\n            'mortality_label': torch.tensor(self.mortality_labels[idx], dtype=torch.float),\n            'adm_type_label': torch.tensor(self.adm_type_labels[idx], dtype=torch.long)\n        }\n\n\n# ===========================================================================\n# MULTI-TASK MODEL\n# ===========================================================================\nclass MTLReadmissionModel(nn.Module):\n    \"\"\"Multi-Task Learning model with shared trunk and three task heads.\"\"\"\n    \n    def __init__(self, model_name, num_admission_types):\n        super().__init__()\n        \n        # Shared trunk: Clinical-Longformer\n        self.backbone = AutoModel.from_pretrained(model_name)\n        hidden_size = self.backbone.config.hidden_size\n        \n        # Task heads\n        self.readmit_head = nn.Linear(hidden_size, 1)  # Binary\n        self.mortality_head = nn.Linear(hidden_size, 1)  # Binary\n        self.adm_type_head = nn.Linear(hidden_size, num_admission_types)  # Multiclass\n        \n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, input_ids, attention_mask, **kwargs):\n        # By adding **kwargs, this function now ignores any\n        # extra keys (like 'readmit_label', 'labels', etc.)\n        # passed to it during the evaluation_loop/prediction_step.\n        \n        # Get [CLS] token representation from backbone\n        outputs = self.backbone(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Use [CLS] token (first token)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        cls_output = self.dropout(cls_output)\n        \n        # Three separate predictions\n        readmit_logits = self.readmit_head(cls_output)\n        mortality_logits = self.mortality_head(cls_output)\n        adm_type_logits = self.adm_type_head(cls_output)\n        \n        return {\n            'readmit_logits': readmit_logits,\n            'mortality_logits': mortality_logits,\n            'adm_type_logits': adm_type_logits\n        }\n\n\n# ===========================================================================\n# CUSTOM TRAINER\n# ===========================================================================\nclass MTLTrainer(Trainer):\n    \"\"\"Custom trainer with multi-task loss.\"\"\"\n    \n    def __init__(self, pos_weight, weight_readmit, weight_mortality, \n                 weight_adm_type, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pos_weight = torch.tensor([pos_weight])\n        self.weight_readmit = weight_readmit\n        self.weight_mortality = weight_mortality\n        self.weight_adm_type = weight_adm_type\n        \n        # Loss functions\n        self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n        self.bce_loss_mortality = nn.BCEWithLogitsLoss()\n        self.ce_loss = nn.CrossEntropyLoss()\n    \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        # Move pos_weight to correct device\n        model_device = next(model.parameters()).device\n        if self.pos_weight.device != model_device:\n            self.pos_weight = self.pos_weight.to(model_device)\n            self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n            \n        # Extract labels\n        readmit_labels = inputs.pop('readmit_label')\n        mortality_labels = inputs.pop('mortality_label')\n        adm_type_labels = inputs.pop('adm_type_label')\n        \n        # Remove the composite 'labels' key, which is only for metrics\n        inputs.pop('labels', None) \n        \n        # Forward pass\n        # 'inputs' now ONLY contains 'input_ids' and 'attention_mask'\n        outputs = model(**inputs)\n        \n        # Calculate individual losses\n        loss_readmit = self.bce_loss(outputs['readmit_logits'].squeeze(),readmit_labels)\n        \n        loss_mortality = self.bce_loss_mortality(outputs['mortality_logits'].squeeze(),mortality_labels)\n        \n        loss_adm_type = self.ce_loss(outputs['adm_type_logits'],adm_type_labels)\n        \n        # Combined loss with weights\n        total_loss = (self.weight_readmit * loss_readmit + \\\n                      self.weight_mortality * loss_mortality + \\\n                      self.weight_adm_type * loss_adm_type)\n        \n        return (total_loss, outputs) if return_outputs else total_loss\n\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        \"\"\"Custom prediction step to return predictions in the correct format.\"\"\"\n        \n        # Extract labels\n        readmit_labels = inputs.get('readmit_label')\n        mortality_labels = inputs.get('mortality_label')\n        adm_type_labels = inputs.get('adm_type_label')\n        \n        # Stack labels for metrics\n        labels = torch.stack([\n            readmit_labels,\n            mortality_labels,\n            adm_type_labels.float()\n        ], dim=1)\n        \n        # Remove individual label keys and composite labels\n        inputs_for_model = {\n            'input_ids': inputs['input_ids'],\n            'attention_mask': inputs['attention_mask']\n        }\n        \n        # Get predictions\n        with torch.no_grad():\n            outputs = model(**inputs_for_model)\n            \n            # Calculate loss\n            loss = self.compute_loss(model, inputs.copy(), return_outputs=False)\n        \n        # Stack predictions as tuple (readmit_logits, mortality_logits, adm_type_logits)\n        predictions = (\n            outputs['readmit_logits'].detach().cpu(),\n            outputs['mortality_logits'].detach().cpu(),\n            outputs['adm_type_logits'].detach().cpu()\n        )\n        \n        if prediction_loss_only:\n            return (loss, None, None)\n        \n        return (loss, predictions, labels.detach().cpu())\n\n        \n# ===========================================================================\n# EVALUATION METRICS\n# ===========================================================================\n    \ndef compute_metrics(eval_pred):\n    \"\"\"Compute metrics for the main task (readmission) only.\"\"\"\n    predictions, labels = eval_pred\n    \n    # predictions is now a tuple: (readmit_logits, mortality_logits, adm_type_logits)\n    # Extract readmission predictions\n    if isinstance(predictions, tuple):\n        readmit_logits = predictions[0]\n    else:\n        readmit_logits = predictions\n    \n    # Extract readmission labels (first column)\n    readmit_labels = labels[:, 0]\n    \n    # Convert logits to probabilities and predictions\n    readmit_logits_np = readmit_logits.numpy() if isinstance(readmit_logits, torch.Tensor) else readmit_logits\n    readmit_probs = 1 / (1 + np.exp(-readmit_logits_np.squeeze()))  # Sigmoid\n    readmit_preds = (readmit_probs > 0.5).astype(int)\n    \n    # Handle edge case: if all predictions are same class\n    try:\n        roc_auc = roc_auc_score(readmit_labels, readmit_probs)\n    except ValueError:\n        roc_auc = 0.0\n    \n    # Calculate metrics\n    metrics = {\n        'roc_auc': roc_auc,\n        'accuracy': accuracy_score(readmit_labels, readmit_preds),\n        'precision': precision_score(readmit_labels, readmit_preds, zero_division=0),\n        'recall': recall_score(readmit_labels, readmit_preds, zero_division=0),\n        'f1': f1_score(readmit_labels, readmit_preds, zero_division=0)\n    }\n    \n    return metrics\n\n\ndef custom_data_collator(features):\n    \"\"\"Custom collator to handle multiple labels.\"\"\"\n    batch = {\n        'input_ids': torch.stack([f['input_ids'] for f in features]),\n        'attention_mask': torch.stack([f['attention_mask'] for f in features]),\n        'readmit_label': torch.stack([f['readmit_label'] for f in features]),\n        'mortality_label': torch.stack([f['mortality_label'] for f in features]),\n        'adm_type_label': torch.stack([f['adm_type_label'] for f in features]),\n    }\n    \n    return batch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:02:44.060088Z","iopub.execute_input":"2025-11-17T21:02:44.060712Z","iopub.status.idle":"2025-11-17T21:02:56.013904Z","shell.execute_reply.started":"2025-11-17T21:02:44.060685Z","shell.execute_reply":"2025-11-17T21:02:56.013267Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 21:02:50.007225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763413370.029461     424 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763413370.036334     424 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===========================================================================\n# MAIN SCRIPT\n# ===========================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MULTI-TASK LEARNING FOR HOSPITAL READMISSION PREDICTION\")\nprint(\"          (BASE MODEL FROZEN - FEATURE EXTRACTION)\")\nprint(\"=\" * 80 + \"\\n\")\n\n# Set random seeds\ntorch.manual_seed(CONFIG.RANDOM_SEED)\nnp.random.seed(CONFIG.RANDOM_SEED)\n\n# Load data\ndf, label_encoder, num_admission_types, pos_weight = load_and_prepare_data(CONFIG)\n\n# Split data\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SPLITTING DATA\")\nprint(\"=\" * 80)\ntrain_df, val_df = train_test_split(\n    df, \n    test_size=CONFIG.TEST_SIZE, \n    random_state=CONFIG.RANDOM_SEED,\n    stratify=df['readmitted_30day']\n)\nprint(f\"‚úì Train size: {len(train_df)}\")\nprint(f\"‚úì Validation size: {len(val_df)}\")\n\n# Load tokenizer\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LOADING TOKENIZER\")\nprint(\"=\" * 80)\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_NAME)\nprint(f\"‚úì Loaded tokenizer: {CONFIG.MODEL_NAME}\")\n\n# Create datasets\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CREATING DATASETS\")\nprint(\"=\" * 80)\ntrain_dataset = MTLDataset(\n    texts=train_df['final_text'].values,\n    readmit_labels=train_df['readmitted_30day'].values,\n    mortality_labels=train_df['hospital_expire_flag'].values,\n    adm_type_labels=train_df['admission_type_encoded'].values,\n    tokenizer=tokenizer,\n    max_length=CONFIG.MAX_LENGTH\n)\n\nval_dataset = MTLDataset(\n    texts=val_df['final_text'].values,\n    readmit_labels=val_df['readmitted_30day'].values,\n    mortality_labels=val_df['hospital_expire_flag'].values,\n    adm_type_labels=val_df['admission_type_encoded'].values,\n    tokenizer=tokenizer,\n    max_length=CONFIG.MAX_LENGTH\n)\nprint(f\"‚úì Train dataset: {len(train_dataset)} samples\")\nprint(f\"‚úì Validation dataset: {len(val_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:02:56.015107Z","iopub.execute_input":"2025-11-17T21:02:56.015671Z","iopub.status.idle":"2025-11-17T21:04:30.556315Z","shell.execute_reply.started":"2025-11-17T21:02:56.015637Z","shell.execute_reply":"2025-11-17T21:04:30.555432Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nMULTI-TASK LEARNING FOR HOSPITAL READMISSION PREDICTION\n          (BASE MODEL FROZEN - FEATURE EXTRACTION)\n================================================================================\n\n================================================================================\nLOADING DATA\n================================================================================\n‚úì Loaded admissions: (374139, 16)\n‚úì Loaded discharge notes: (331731, 7)\n‚úì Loaded radiology notes: (1144023, 7)\n\nCombining notes...\n‚úì Final merged data: (374139, 18)\n‚úì Sampled 100 rows for testing\n\nPreparing labels...\n  - Readmission distribution: {0: 79, 1: 21}\n  - Mortality distribution: {0: 97, 1: 3}\n  - Admission types: 9 classes\n  - Positive weight for readmission: 3.76\n\n================================================================================\nSPLITTING DATA\n================================================================================\n‚úì Train size: 80\n‚úì Validation size: 20\n\n================================================================================\nLOADING TOKENIZER\n================================================================================\n‚úì Loaded tokenizer: yikuan8/Clinical-Longformer\n\n================================================================================\nCREATING DATASETS\n================================================================================\n‚úì Train dataset: 80 samples\n‚úì Validation dataset: 20 samples\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# Initialize model\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INITIALIZING MODEL\")\nprint(\"=\" * 80)\nmodel = MTLReadmissionModel(CONFIG.MODEL_NAME, num_admission_types)\nprint(f\"‚úì Model created with {num_admission_types} admission type classes\")\n\n\n# ---\n# --- ‚ùÑÔ∏è START: USER-REQUESTED CHANGE (FREEZE BASE MODEL) ‚ùÑÔ∏è ---\n# ---\nprint(\"\\n\" + \"-\" * 80)\nprint(\"‚ùÑÔ∏è FREEZING BASE MODEL PARAMETERS (Clinical-Longformer)...\")\nfor param in model.backbone.parameters():\n    param.requires_grad = False\nprint(\"‚úì Base model frozen. Only the classification heads will be fine-tuned.\")\nprint(\"-\" * 80)\n# ---\n# --- ‚ùÑÔ∏è END: USER-REQUESTED CHANGE ‚ùÑÔ∏è ---\n# ---\n\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=CONFIG.OUTPUT_DIR,\n    num_train_epochs=CONFIG.EPOCHS,\n    per_device_train_batch_size=CONFIG.TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=CONFIG.VALID_BATCH_SIZE,\n    gradient_accumulation_steps=CONFIG.GRADIENT_ACCUMULATION,\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    warmup_ratio=CONFIG.WARMUP_RATIO,\n    fp16=CONFIG.USE_FP16,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"roc_auc\",\n    greater_is_better=True,\n    save_total_limit=2,\n    report_to=\"none\",\n    seed=CONFIG.RANDOM_SEED,\n    remove_unused_columns=False,\n)\n\n# Initialize trainer\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INITIALIZING TRAINER\")\nprint(\"=\" * 80)\ntrainer = MTLTrainer(\n    pos_weight=pos_weight,\n    weight_readmit=CONFIG.WEIGHT_READMIT,\n    weight_mortality=CONFIG.WEIGHT_MORTALITY,\n    weight_adm_type=CONFIG.WEIGHT_ADM_TYPE,\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=custom_data_collator,\n    compute_metrics=compute_metrics,\n)\nprint(\"‚úì Trainer initialized\")\n\n# Train\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING MODEL (ONLY HEADS)\")\nprint(\"=\" * 80)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:04:30.557228Z","iopub.execute_input":"2025-11-17T21:04:30.557507Z","iopub.status.idle":"2025-11-17T21:05:14.908093Z","shell.execute_reply.started":"2025-11-17T21:04:30.557481Z","shell.execute_reply":"2025-11-17T21:05:14.907365Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nINITIALIZING MODEL\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/595M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee34e1fa95345b1a5058ba89b379349"}},"metadata":{}},{"name":"stderr","text":"Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úì Model created with 9 admission type classes\n\n--------------------------------------------------------------------------------\n‚ùÑÔ∏è FREEZING BASE MODEL PARAMETERS (Clinical-Longformer)...\n‚úì Base model frozen. Only the classification heads will be fine-tuned.\n--------------------------------------------------------------------------------\n\n================================================================================\nINITIALIZING TRAINER\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/595M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaed7b1f5cfd493799a575d8b5894686"}},"metadata":{}},{"name":"stdout","text":"‚úì Trainer initialized\n\n================================================================================\nTRAINING MODEL (ONLY HEADS)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:27, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Roc Auc</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.500375</td>\n      <td>0.578125</td>\n      <td>0.200000</td>\n      <td>0.200000</td>\n      <td>1.000000</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3, training_loss=2.692969640096029, metrics={'train_runtime': 39.9602, 'train_samples_per_second': 2.002, 'train_steps_per_second': 0.075, 'total_flos': 0.0, 'train_loss': 2.692969640096029, 'epoch': 1.0})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Final evaluation\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINAL EVALUATION (MAIN TASK: 30-DAY READMISSION)\")\nprint(\"=\" * 80)\neval_results = trainer.evaluate()\n\nprint(\"\\nüìä RESULTS (BASE MODEL FROZEN):\")\nprint(f\"  ROC-AUC:   {eval_results['eval_roc_auc']:.4f}\")\nprint(f\"  Accuracy:  {eval_results['eval_accuracy']:.4f}\")\nprint(f\"  Precision: {eval_results['eval_precision']:.4f}\")\nprint(f\"  Recall:    {eval_results['eval_recall']:.4f}\")\nprint(f\"  F1 Score:  {eval_results['eval_f1']:.4f}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Model saved to: {CONFIG.OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:05:14.908786Z","iopub.execute_input":"2025-11-17T21:05:14.909038Z","iopub.status.idle":"2025-11-17T21:05:25.853266Z","shell.execute_reply.started":"2025-11-17T21:05:14.909010Z","shell.execute_reply":"2025-11-17T21:05:25.852641Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nFINAL EVALUATION (MAIN TASK: 30-DAY READMISSION)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nüìä RESULTS (BASE MODEL FROZEN):\n  ROC-AUC:   0.5781\n  Accuracy:  0.2000\n  Precision: 0.2000\n  Recall:    1.0000\n  F1 Score:  0.3333\n\n================================================================================\nTRAINING COMPLETE!\n================================================================================\nModel saved to: ./mtl_readmission_model_FROZEN\n","output_type":"stream"}],"execution_count":5}]}