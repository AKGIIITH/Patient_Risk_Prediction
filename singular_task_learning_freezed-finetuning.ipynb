{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13769531,"sourceType":"datasetVersion","datasetId":8763276}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    Trainer,\n    TrainingArguments,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score\n)\nfrom huggingface_hub import login, HfApi\nimport warnings\nimport logging\nimport os\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nos.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n\nprint(\"âœ“ All imports successful\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T00:05:51.188367Z","iopub.execute_input":"2025-11-18T00:05:51.188876Z","iopub.status.idle":"2025-11-18T00:05:51.194912Z","shell.execute_reply.started":"2025-11-18T00:05:51.188854Z","shell.execute_reply":"2025-11-18T00:05:51.194061Z"}},"outputs":[{"name":"stdout","text":"âœ“ All imports successful\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# HUGGING FACE CONFIGURATION\n# ============================================================================\n\nclass HF_CONFIG:\n    \"\"\"Hugging Face Hub configuration\"\"\"\n    \n    # Your Hugging Face username and model repository name\n    HUB_USERNAME = \"AKG2\"  # â† Change this to your HF username\n    HUB_MODEL_NAME = \"clinical-longformer-readmission-frozen\"  # â† Different name for frozen model\n    \n    # This will be constructed as: username/model-name\n    @property\n    def HUB_MODEL_ID(self):\n        return f\"{self.HUB_USERNAME}/{self.HUB_MODEL_NAME}\"\n    \n    # Kaggle secret name for HF token (don't change unless you named it differently)\n    KAGGLE_SECRET_NAME = \"HF_TOKEN\"\n    \n    # Whether to push to hub during training\n    PUSH_TO_HUB = True\n    \n    # Whether to make the repository private\n    PRIVATE_REPO = False\n\n# ============================================================================\n# TRAINING CONFIGURATION\n# ============================================================================\n\nclass CONFIG:\n    \"\"\"Training configuration\"\"\"\n    \n    # Data paths\n    DATA_PATH = \"/kaggle/input/mimic-iv\"\n    MODEL_NAME = \"yikuan8/Clinical-Longformer\"\n    \n    # Hugging Face configuration\n    HF = HF_CONFIG()\n    \n    # Dataset settings\n    SAMPLE_SIZE = 30000  # Set to None to use full dataset\n    TEST_SIZE = 0.2\n    RANDOM_SEED = 42\n    \n    # Model settings\n    MAX_LENGTH = 4096\n    FREEZE_BACKBONE = True  # â† NEW: Freeze backbone weights\n    \n    # Training hyperparameters (adjusted for frozen backbone)\n    EPOCHS = 10  # Can train longer since only classifier is updating\n    TRAIN_BATCH_SIZE = 4  # Can use larger batch size\n    VALID_BATCH_SIZE = 8\n    GRADIENT_ACCUMULATION = 8  # Reduced since we have larger batch size\n    LEARNING_RATE = 1e-3  # Higher LR for classifier-only training\n    WEIGHT_DECAY = 0.01\n    WARMUP_RATIO = 0.1\n    \n    # GPU optimization\n    USE_FP16 = True\n    \n    # Output directory\n    OUTPUT_DIR = \"./frozen_readmission_model\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T00:05:51.201622Z","iopub.execute_input":"2025-11-18T00:05:51.201938Z","iopub.status.idle":"2025-11-18T00:05:51.212881Z","shell.execute_reply.started":"2025-11-18T00:05:51.201917Z","shell.execute_reply":"2025-11-18T00:05:51.212179Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# HUGGING FACE HUB LOGIN\n# ============================================================================\n\ndef login_to_huggingface(config):\n    \"\"\"Log in to Hugging Face Hub using Kaggle secret\"\"\"\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        hf_token = user_secrets.get_secret(config.HF.KAGGLE_SECRET_NAME)\n        \n        if hf_token:\n            login(token=hf_token, add_to_git_credential=True)\n            print(f\"âœ“ Successfully logged in to Hugging Face Hub\")\n            print(f\"  Model will be pushed to: {config.HF.HUB_MODEL_ID}\")\n            return True\n        else:\n            print(\"âš  HF_TOKEN not found in Kaggle secrets\")\n            return False\n    except Exception as e:\n        print(f\"âš  Could not log in to Hugging Face Hub: {e}\")\n        print(\"  Training will continue without Hub integration\")\n        return False\n\n\n# ============================================================================\n# DATA LOADING AND PREPARATION\n# ============================================================================\n\ndef load_and_prepare_data(config):\n    \"\"\"Load and merge all data sources for readmission prediction\"\"\"\n    print(\"=\" * 80)\n    print(\"LOADING DATA\")\n    print(\"=\" * 80)\n    \n    # Load main admissions file\n    admissions = pd.read_csv(f\"{config.DATA_PATH}/admissions_with_readmission_labels.csv\")\n    print(f\"âœ“ Loaded admissions: {admissions.shape}\")\n    \n    # Load discharge notes\n    discharge = pd.read_csv(f\"{config.DATA_PATH}/discharge_notes-001.csv\")\n    print(f\"âœ“ Loaded discharge notes: {discharge.shape}\")\n    \n    # Load radiology notes\n    radiology = pd.read_csv(f\"{config.DATA_PATH}/radiology_notes.csv\")\n    print(f\"âœ“ Loaded radiology notes: {radiology.shape}\")\n    \n    # Combine notes: concatenate discharge and radiology by hadm_id\n    print(\"\\nCombining notes...\")\n    discharge_grouped = discharge.groupby('hadm_id')['text'].apply(\n        lambda x: ' '.join(x.astype(str))\n    ).reset_index()\n    discharge_grouped.columns = ['hadm_id', 'discharge_text']\n    \n    radiology_grouped = radiology.groupby('hadm_id')['text'].apply(\n        lambda x: ' '.join(x.astype(str))\n    ).reset_index()\n    radiology_grouped.columns = ['hadm_id', 'radiology_text']\n    \n    # Merge notes\n    notes_combined = discharge_grouped.merge(\n        radiology_grouped, on='hadm_id', how='outer'\n    )\n    \n    # Combine all text\n    notes_combined['combined_text'] = (\n        notes_combined['discharge_text'].fillna('') + ' ' + \n        notes_combined['radiology_text'].fillna('')\n    )\n    notes_combined['combined_text'] = notes_combined['combined_text'].str.strip()\n    \n    # Merge with admissions\n    df = admissions.merge(notes_combined[['hadm_id', 'combined_text']], \n                          on='hadm_id', how='left')\n    \n    # Use combined_text if available, otherwise use original text\n    df['final_text'] = df['combined_text'].fillna(df.get('text', ''))\n    df['final_text'] = df['final_text'].fillna('')\n    \n    print(f\"âœ“ Final merged data: {df.shape}\")\n    \n    # Sample data if needed\n    if config.SAMPLE_SIZE is not None:\n        df = df.sample(n=min(config.SAMPLE_SIZE, len(df)), \n                       random_state=config.RANDOM_SEED)\n        print(f\"âœ“ Sampled {len(df)} rows for testing\")\n    \n    # Prepare readmission labels (binary: 0 or 1)\n    print(\"\\nPreparing labels...\")\n    df['readmitted_30day'] = df['readmitted_30day'].astype(int)\n    \n    print(f\"  - Readmission distribution: {df['readmitted_30day'].value_counts().to_dict()}\")\n    \n    # Calculate class weights for readmission (handle imbalance)\n    pos_count = df['readmitted_30day'].sum()\n    neg_count = len(df) - pos_count\n    pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n    \n    print(f\"  - Positive weight for readmission: {pos_weight:.2f}\")\n    \n    return df, pos_weight\n\n\n# ============================================================================\n# DATASET CLASS\n# ============================================================================\n\nclass ReadmissionDataset(Dataset):\n    \"\"\"Dataset for single-task readmission prediction\"\"\"\n    \n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        \n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n        }\n\n\n# ============================================================================\n# FROZEN BACKBONE MODEL\n# ============================================================================\n\nclass FrozenReadmissionModel(nn.Module):\n    \"\"\"Model with frozen backbone - only classifier is trainable\"\"\"\n    \n    def __init__(self, model_name, freeze_backbone=True):\n        super().__init__()\n        \n        # Backbone: Clinical-Longformer\n        self.backbone = AutoModel.from_pretrained(model_name)\n        hidden_size = self.backbone.config.hidden_size\n        \n        # Freeze backbone if specified\n        if freeze_backbone:\n            print(\"\\nðŸ”’ FREEZING BACKBONE WEIGHTS\")\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n            \n            # Count frozen parameters\n            frozen_params = sum(p.numel() for p in self.backbone.parameters())\n            print(f\"  - Frozen parameters: {frozen_params:,}\")\n        \n        # Single classification head for readmission (TRAINABLE)\n        self.classifier = nn.Linear(hidden_size, 1)\n        \n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.1)\n        \n        # Count trainable parameters\n        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        total_params = sum(p.numel() for p in self.parameters())\n        print(f\"  - Trainable parameters: {trainable_params:,}\")\n        print(f\"  - Total parameters: {total_params:,}\")\n        print(f\"  - Trainable %: {100 * trainable_params / total_params:.2f}%\")\n    \n    def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n        # Get [CLS] token representation from backbone (no gradient)\n        with torch.set_grad_enabled(self.training):\n            outputs = self.backbone(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n        \n        # Use [CLS] token (first token)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        cls_output = self.dropout(cls_output)\n        \n        # Readmission prediction (this layer is trainable)\n        logits = self.classifier(cls_output)\n        \n        return {'logits': logits}\n\n\n# ============================================================================\n# CUSTOM TRAINER WITH WEIGHTED LOSS\n# ============================================================================\n\nclass WeightedLossTrainer(Trainer):\n    \"\"\"Custom trainer with weighted BCE loss for class imbalance\"\"\"\n    \n    def __init__(self, pos_weight, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pos_weight = torch.tensor([pos_weight])\n        self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n    \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        # Move pos_weight to correct device\n        model_device = next(model.parameters()).device\n        if self.pos_weight.device != model_device:\n            self.pos_weight = self.pos_weight.to(model_device)\n            self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n        \n        # Extract labels\n        labels = inputs.pop('labels')\n        \n        # Forward pass\n        outputs = model(**inputs)\n        \n        # Calculate loss\n        loss = self.bce_loss(outputs['logits'].squeeze(), labels)\n        \n        return (loss, outputs) if return_outputs else loss\n\n\n# ============================================================================\n# EVALUATION METRICS\n# ============================================================================\n\ndef compute_metrics(eval_pred):\n    \"\"\"Compute metrics for readmission prediction\"\"\"\n    predictions, labels = eval_pred\n    \n    # Extract logits (handle both dict and tensor formats)\n    if isinstance(predictions, dict):\n        logits = predictions['logits']\n    elif isinstance(predictions, tuple):\n        logits = predictions[0]\n    else:\n        logits = predictions\n    \n    # Convert to numpy if needed\n    logits_np = logits.numpy() if isinstance(logits, torch.Tensor) else logits\n    \n    # Convert logits to probabilities (sigmoid)\n    probs = 1 / (1 + np.exp(-logits_np.squeeze()))\n    preds = (probs > 0.5).astype(int)\n    \n    # Handle edge case: if all predictions are same class\n    try:\n        roc_auc = roc_auc_score(labels, probs)\n    except ValueError:\n        roc_auc = 0.0\n    \n    # Calculate metrics\n    metrics = {\n        'roc_auc': roc_auc,\n        'accuracy': accuracy_score(labels, preds),\n        'precision': precision_score(labels, preds, zero_division=0),\n        'recall': recall_score(labels, preds, zero_division=0),\n        'f1': f1_score(labels, preds, zero_division=0)\n    }\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T00:05:51.297448Z","iopub.execute_input":"2025-11-18T00:05:51.297735Z","iopub.status.idle":"2025-11-18T00:05:51.319517Z","shell.execute_reply.started":"2025-11-18T00:05:51.297711Z","shell.execute_reply":"2025-11-18T00:05:51.318879Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# MAIN TRAINING SCRIPT\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FROZEN BACKBONE FINE-TUNING FOR HOSPITAL READMISSION PREDICTION\")\nprint(\"=\" * 80 + \"\\n\")\n\n# Set random seeds\ntorch.manual_seed(CONFIG.RANDOM_SEED)\nnp.random.seed(CONFIG.RANDOM_SEED)\n\n# Login to Hugging Face Hub\nhf_logged_in = login_to_huggingface(CONFIG)\n\n# Load data\ndf, pos_weight = load_and_prepare_data(CONFIG)\n\n# Split data\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SPLITTING DATA\")\nprint(\"=\" * 80)\ntrain_df, val_df = train_test_split(\n    df, \n    test_size=CONFIG.TEST_SIZE, \n    random_state=CONFIG.RANDOM_SEED,\n    stratify=df['readmitted_30day']\n)\nprint(f\"âœ“ Train size: {len(train_df)}\")\nprint(f\"âœ“ Validation size: {len(val_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T00:05:51.320689Z","iopub.execute_input":"2025-11-18T00:05:51.321002Z","iopub.status.idle":"2025-11-18T00:07:27.757193Z","shell.execute_reply.started":"2025-11-18T00:05:51.320984Z","shell.execute_reply":"2025-11-18T00:07:27.756562Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nFROZEN BACKBONE FINE-TUNING FOR HOSPITAL READMISSION PREDICTION\n================================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Token has not been saved to git credential helper.\nWARNING:huggingface_hub._login:Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nâœ“ Successfully logged in to Hugging Face Hub\n  Model will be pushed to: AKG2/clinical-longformer-readmission-frozen\n================================================================================\nLOADING DATA\n================================================================================\nâœ“ Loaded admissions: (374139, 16)\nâœ“ Loaded discharge notes: (331731, 7)\nâœ“ Loaded radiology notes: (1144023, 7)\n\nCombining notes...\nâœ“ Final merged data: (374139, 18)\nâœ“ Sampled 100 rows for testing\n\nPreparing labels...\n  - Readmission distribution: {0: 79, 1: 21}\n  - Positive weight for readmission: 3.76\n\n================================================================================\nSPLITTING DATA\n================================================================================\nâœ“ Train size: 80\nâœ“ Validation size: 20\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Load tokenizer\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LOADING TOKENIZER\")\nprint(\"=\" * 80)\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.MODEL_NAME)\nprint(f\"âœ“ Loaded tokenizer: {CONFIG.MODEL_NAME}\")\n\n# Create datasets\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CREATING DATASETS\")\nprint(\"=\" * 80)\ntrain_dataset = ReadmissionDataset(\n    texts=train_df['final_text'].values,\n    labels=train_df['readmitted_30day'].values,\n    tokenizer=tokenizer,\n    max_length=CONFIG.MAX_LENGTH\n)\n\nval_dataset = ReadmissionDataset(\n    texts=val_df['final_text'].values,\n    labels=val_df['readmitted_30day'].values,\n    tokenizer=tokenizer,\n    max_length=CONFIG.MAX_LENGTH\n)\nprint(f\"âœ“ Train dataset: {len(train_dataset)} samples\")\nprint(f\"âœ“ Validation dataset: {len(val_dataset)} samples\")\n\n# Initialize model with frozen backbone\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INITIALIZING MODEL\")\nprint(\"=\" * 80)\nmodel = FrozenReadmissionModel(CONFIG.MODEL_NAME, freeze_backbone=CONFIG.FREEZE_BACKBONE)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=CONFIG.OUTPUT_DIR,\n    num_train_epochs=CONFIG.EPOCHS,\n    per_device_train_batch_size=CONFIG.TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=CONFIG.VALID_BATCH_SIZE,\n    gradient_accumulation_steps=CONFIG.GRADIENT_ACCUMULATION,\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    warmup_ratio=CONFIG.WARMUP_RATIO,\n    fp16=CONFIG.USE_FP16,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"roc_auc\",\n    greater_is_better=True,\n    save_total_limit=2,\n    report_to=\"none\",\n    seed=CONFIG.RANDOM_SEED,\n    disable_tqdm=True,\n    # Hugging Face Hub settings\n    push_to_hub=CONFIG.HF.PUSH_TO_HUB and hf_logged_in,\n    hub_model_id=CONFIG.HF.HUB_MODEL_ID if hf_logged_in else None,\n    hub_strategy=\"every_save\",\n    hub_private_repo=CONFIG.HF.PRIVATE_REPO,\n)\n\n# Initialize trainer\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INITIALIZING TRAINER\")\nprint(\"=\" * 80)\ntrainer = WeightedLossTrainer(\n    pos_weight=pos_weight,\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\nprint(\"âœ“ Trainer initialized\")\n\n# Train\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING MODEL (CLASSIFIER ONLY)\")\nprint(\"=\" * 80)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T00:07:27.758087Z","iopub.execute_input":"2025-11-18T00:07:27.758349Z","iopub.status.idle":"2025-11-18T00:08:30.209492Z","shell.execute_reply.started":"2025-11-18T00:07:27.758327Z","shell.execute_reply":"2025-11-18T00:08:30.208713Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nLOADING TOKENIZER\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60094bd1b8fb457ab3d5cc0d7c55d993"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fea64fe3bd64a999e3d95138d85c24d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"684bcceff4634a618f2ccac5a807a401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e12f20c3c174e36969158c2b1178376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff1c4cae889b42ae854d60bba18282ad"}},"metadata":{}},{"name":"stdout","text":"âœ“ Loaded tokenizer: yikuan8/Clinical-Longformer\n\n================================================================================\nCREATING DATASETS\n================================================================================\nâœ“ Train dataset: 80 samples\nâœ“ Validation dataset: 20 samples\n\n================================================================================\nINITIALIZING MODEL\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79f827c57a9b40deb9d495251e3fde4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/595M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"897a58739f6e43968e8136e48ead9443"}},"metadata":{}},{"name":"stdout","text":"\nðŸ”’ FREEZING BACKBONE WEIGHTS\n  - Frozen parameters: 148,659,456\n  - Trainable parameters: 769\n  - Total parameters: 148,660,225\n  - Trainable %: 0.00%\n\n================================================================================\nINITIALIZING TRAINER\n================================================================================\nâœ“ Trainer initialized\n\n================================================================================\nTRAINING MODEL (CLASSIFIER ONLY)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:22, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Roc Auc</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.080237</td>\n      <td>0.578125</td>\n      <td>0.200000</td>\n      <td>0.200000</td>\n      <td>1.000000</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3, training_loss=7.447104136149089, metrics={'train_runtime': 31.837, 'train_samples_per_second': 2.513, 'train_steps_per_second': 0.094, 'total_flos': 0.0, 'train_loss': 7.447104136149089, 'epoch': 1.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Final evaluation\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINAL EVALUATION (30-DAY READMISSION)\")\nprint(\"=\" * 80)\neval_results = trainer.evaluate()\n\nprint(\"\\nðŸ“Š RESULTS:\")\nprint(f\"  ROC-AUC:   {eval_results['eval_roc_auc']:.4f}\")\nprint(f\"  Accuracy:  {eval_results['eval_accuracy']:.4f}\")\nprint(f\"  Precision: {eval_results['eval_precision']:.4f}\")\nprint(f\"  Recall:    {eval_results['eval_recall']:.4f}\")\nprint(f\"  F1 Score:  {eval_results['eval_f1']:.4f}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T00:08:30.210385Z","iopub.execute_input":"2025-11-18T00:08:30.210656Z","iopub.status.idle":"2025-11-18T00:08:35.719571Z","shell.execute_reply.started":"2025-11-18T00:08:30.210634Z","shell.execute_reply":"2025-11-18T00:08:35.718816Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nFINAL EVALUATION (30-DAY READMISSION)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nðŸ“Š RESULTS:\n  ROC-AUC:   0.5781\n  Accuracy:  0.2000\n  Precision: 0.2000\n  Recall:    1.0000\n  F1 Score:  0.3333\n\n================================================================================\nTRAINING COMPLETE!\n================================================================================\n","output_type":"stream"}],"execution_count":15}]}