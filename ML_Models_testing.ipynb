{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fuR61udcHue",
        "outputId": "d1eb5233-6e7e-4efd-f286-8af6a04f6543"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXTaDXuyYJug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c2c3c8-963e-45f4-e43a-bbb9e17f736b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MEMORY OPTIMIZATION SETTINGS\n",
            "============================================================\n",
            "Sample size: 150000\n",
            "Max TF-IDF features: 20000\n",
            "\n",
            "============================================================\n",
            "STEP 1: Loading Data\n",
            "============================================================\n",
            "Admissions data shape: (374139, 2)\n",
            "Loading discharge notes...\n",
            "Loading radiology notes...\n",
            "Discharge notes shape: (331731, 2)\n",
            "Radiology notes shape: (1144023, 2)\n",
            "\n",
            "============================================================\n",
            "STEP 2: Preparing Text Features (Memory Efficient)\n",
            "============================================================\n",
            "Combined notes shape: (1475754, 2)\n",
            "Aggregating notes by admission...\n",
            "Aggregated notes shape: (374139, 2)\n",
            "\n",
            "============================================================\n",
            "STEP 3: Preparing Final Dataset\n",
            "============================================================\n",
            "Final dataset shape: (374139, 3)\n",
            "\n",
            "‚ö†Ô∏è  Using stratified sample of 150000 records for memory efficiency\n",
            "Sampled dataset shape: (149999, 3)\n",
            "Readmission distribution:\n",
            "readmitted_30day\n",
            "0    119641\n",
            "1     30358\n",
            "Name: count, dtype: int64\n",
            "Readmission rate: 20.24%\n",
            "\n",
            "============================================================\n",
            "STEP 4: Defining Features and Target\n",
            "============================================================\n",
            "X shape: (149999,)\n",
            "y shape: (149999,)\n",
            "\n",
            "============================================================\n",
            "STEP 5: Train-Test Split\n",
            "============================================================\n",
            "Training set size: 119999\n",
            "Test set size: 30000\n",
            "Training readmission rate: 20.24%\n",
            "Test readmission rate: 20.24%\n",
            "\n",
            "============================================================\n",
            "STEP 6: TF-IDF Vectorization (Optimized)\n",
            "============================================================\n",
            "Fitting TF-IDF vectorizer on training data...\n",
            "Transforming test data...\n",
            "TF-IDF training matrix shape: (119999, 20000)\n",
            "TF-IDF test matrix shape: (30000, 20000)\n",
            "\n",
            "============================================================\n",
            "STEP 7: Training and Evaluating All Models\n",
            "============================================================\n",
            "\n",
            "üíæ Output file initialized: output.txt\n",
            "\n",
            "üîÑ Training Logistic Regression...\n",
            "============================================================\n",
            "MODEL: Logistic Regression\n",
            "============================================================\n",
            "\n",
            "üìä CLASSIFICATION REPORT:\n",
            "------------------------------------------------------------\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Not Readmitted       0.87      0.69      0.77     23928\n",
            "    Readmitted       0.33      0.59      0.42      6072\n",
            "\n",
            "      accuracy                           0.67     30000\n",
            "     macro avg       0.60      0.64      0.60     30000\n",
            "  weighted avg       0.76      0.67      0.70     30000\n",
            "\n",
            "\n",
            "üìà CONFUSION MATRIX:\n",
            "------------------------------------------------------------\n",
            "[[16570  7358]\n",
            " [ 2481  3591]]\n",
            "True Negatives:  16570\n",
            "False Positives: 7358\n",
            "False Negatives: 2481\n",
            "True Positives:  3591\n",
            "\n",
            "üéØ ROC-AUC SCORE: 0.6971\n",
            "\n",
            "üìå SUMMARY METRICS:\n",
            "------------------------------------------------------------\n",
            "Accuracy:  0.6720\n",
            "Precision: 0.3280\n",
            "Recall:    0.5914\n",
            "F1-Score:  0.4219\n",
            "ROC-AUC:   0.6971\n",
            "\n",
            "üîù TOP 15 MOST PREDICTIVE FEATURES:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Most predictive of READMISSION:\n",
            "  admissions: 5.3041\n",
            "  discharged: 3.0351\n",
            "  gastroparesis: 2.9407\n",
            "  ama: 2.9094\n",
            "  intoxication: 2.7360\n",
            "  lymphoma: 2.6757\n",
            "  desensitization: 2.6541\n",
            "  personality: 2.5501\n",
            "  schizophrenia: 2.4403\n",
            "  sarcoma: 2.3744\n",
            "  carotids: 2.3216\n",
            "  osteosarcoma: 2.3121\n",
            "  recently: 2.3044\n",
            "  dilaudid: 2.2716\n",
            "  scheduled: 2.2278\n",
            "\n",
            "Most predictive of NO READMISSION:\n",
            "  expired: -6.0740\n",
            "  hospice: -3.5414\n",
            "  comfort: -3.1110\n",
            "  file: -2.8777\n",
            "  cmo: -2.6989\n",
            "  nonobstructing: -2.5703\n",
            "  adrs: -2.4900\n",
            "  anasarca: -2.4785\n",
            "  osh: -2.4076\n",
            "  year: -2.4017\n",
            "  periorbital: -2.3536\n",
            "  postpartum: -2.3110\n",
            "  maintenance: -2.2776\n",
            "  adverse: -2.2129\n",
            "  available: -2.1445\n",
            "\n",
            "============================================================\n",
            "\n",
            "‚úÖ Logistic Regression complete and saved!\n",
            "\n",
            "üîÑ Training Linear SVM...\n",
            "============================================================\n",
            "MODEL: Linear SVM (LinearSVC)\n",
            "============================================================\n",
            "\n",
            "üìä CLASSIFICATION REPORT:\n",
            "------------------------------------------------------------\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Not Readmitted       0.86      0.69      0.77     23928\n",
            "    Readmitted       0.31      0.55      0.40      6072\n",
            "\n",
            "      accuracy                           0.66     30000\n",
            "     macro avg       0.58      0.62      0.58     30000\n",
            "  weighted avg       0.75      0.66      0.69     30000\n",
            "\n",
            "\n",
            "üìà CONFUSION MATRIX:\n",
            "------------------------------------------------------------\n",
            "[[16539  7389]\n",
            " [ 2733  3339]]\n",
            "True Negatives:  16539\n",
            "False Positives: 7389\n",
            "False Negatives: 2733\n",
            "True Positives:  3339\n",
            "\n",
            "üéØ ROC-AUC SCORE: 0.6672\n",
            "\n",
            "üìå SUMMARY METRICS:\n",
            "------------------------------------------------------------\n",
            "Accuracy:  0.6626\n",
            "Precision: 0.3112\n",
            "Recall:    0.5499\n",
            "F1-Score:  0.3975\n",
            "ROC-AUC:   0.6672\n",
            "\n",
            "üîù TOP 15 MOST PREDICTIVE FEATURES:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Most predictive of READMISSION:\n",
            "  admissions: 2.8688\n",
            "  anuerysm: 2.7535\n",
            "  ellipta: 2.7299\n",
            "  bacteremic: 2.7243\n",
            "  blades: 2.6034\n",
            "  509: 2.5445\n",
            "  arousal: 2.5169\n",
            "  bkas: 2.4698\n",
            "  pedicled: 2.4605\n",
            "  1po: 2.4493\n",
            "  derangement: 2.4334\n",
            "  peaking: 2.4100\n",
            "  restored: 2.4098\n",
            "  neur: 2.3711\n",
            "  esoph: 2.3256\n",
            "\n",
            "Most predictive of NO READMISSION:\n",
            "  expired: -6.6092\n",
            "  adrs: -3.1171\n",
            "  considerably: -2.8577\n",
            "  file: -2.6380\n",
            "  encourage: -2.6119\n",
            "  cmo: -2.5785\n",
            "  magnet: -2.5224\n",
            "  smelled: -2.5186\n",
            "  2nd: -2.4469\n",
            "  comfort: -2.4394\n",
            "  calculations: -2.4012\n",
            "  intrafissural: -2.3738\n",
            "  ventilatory: -2.3617\n",
            "  ganglion: -2.3520\n",
            "  seizing: -2.3046\n",
            "\n",
            "============================================================\n",
            "\n",
            "‚úÖ Linear SVM complete and saved!\n",
            "\n",
            "üîÑ Training Random Forest...\n",
            "============================================================\n",
            "MODEL: Random Forest\n",
            "============================================================\n",
            "\n",
            "üìä CLASSIFICATION REPORT:\n",
            "------------------------------------------------------------\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Not Readmitted       0.85      0.76      0.80     23928\n",
            "    Readmitted       0.34      0.48      0.39      6072\n",
            "\n",
            "      accuracy                           0.70     30000\n",
            "     macro avg       0.59      0.62      0.60     30000\n",
            "  weighted avg       0.75      0.70      0.72     30000\n",
            "\n",
            "\n",
            "üìà CONFUSION MATRIX:\n",
            "------------------------------------------------------------\n",
            "[[18183  5745]\n",
            " [ 3174  2898]]\n",
            "True Negatives:  18183\n",
            "False Positives: 5745\n",
            "False Negatives: 3174\n",
            "True Positives:  2898\n",
            "\n",
            "üéØ ROC-AUC SCORE: 0.6697\n",
            "\n",
            "üìå SUMMARY METRICS:\n",
            "------------------------------------------------------------\n",
            "Accuracy:  0.7027\n",
            "Precision: 0.3353\n",
            "Recall:    0.4773\n",
            "F1-Score:  0.3939\n",
            "ROC-AUC:   0.6697\n",
            "\n",
            "üîù TOP 15 MOST PREDICTIVE FEATURES:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Top 15 Most Important Features:\n",
            "  lymphoma: 0.0061\n",
            "  vein: 0.0058\n",
            "  oncologic: 0.0058\n",
            "  complicated: 0.0056\n",
            "  transplant: 0.0048\n",
            "  recently: 0.0046\n",
            "  medications: 0.0045\n",
            "  adverse: 0.0039\n",
            "  biopsy: 0.0038\n",
            "  coherent: 0.0038\n",
            "  multiple: 0.0037\n",
            "  admission: 0.0034\n",
            "  allergies: 0.0033\n",
            "  admitted: 0.0033\n",
            "  chemotherapy: 0.0033\n",
            "\n",
            "============================================================\n",
            "\n",
            "‚úÖ Random Forest complete and saved!\n",
            "\n",
            "üîÑ Training XGBoost...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# MEMORY OPTIMIZATION SETTINGS\n",
        "print(\"=\" * 60)\n",
        "print(\"MEMORY OPTIMIZATION SETTINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Reduce sample size for faster processing (adjust as needed)\n",
        "SAMPLE_SIZE = 150000  # Use subset of data\n",
        "USE_SAMPLING = True  # Set to False to use full dataset\n",
        "\n",
        "# Reduce TF-IDF features\n",
        "MAX_FEATURES = 20000\n",
        "MAX_DF = 0.90  # Filter very common terms\n",
        "MIN_DF = 10  # Filter very rare terms\n",
        "\n",
        "print(f\"Sample size: {SAMPLE_SIZE if USE_SAMPLING else 'Full dataset'}\")\n",
        "print(f\"Max TF-IDF features: {MAX_FEATURES}\")\n",
        "print()\n",
        "\n",
        "# 1. LOAD DATA WITH MEMORY OPTIMIZATION\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Loading Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/mimic_readmission_data\"\n",
        "\n",
        "# Load admissions with only needed columns\n",
        "df_admissions = pd.read_csv(\n",
        "    f\"{data_path}/admissions_with_readmission_labels.csv\",\n",
        "    usecols=['hadm_id', 'readmitted_30day']\n",
        ")\n",
        "\n",
        "print(f\"Admissions data shape: {df_admissions.shape}\")\n",
        "\n",
        "# Load notes in chunks and process efficiently\n",
        "print(\"Loading discharge notes...\")\n",
        "df_discharge = pd.read_csv(\n",
        "    f\"{data_path}/discharge_notes.csv\",\n",
        "    usecols=['hadm_id', 'text']\n",
        ")\n",
        "\n",
        "print(\"Loading radiology notes...\")\n",
        "df_radiology = pd.read_csv(\n",
        "    f\"{data_path}/radiology_notes.csv\",\n",
        "    usecols=['hadm_id', 'text']\n",
        ")\n",
        "\n",
        "print(f\"Discharge notes shape: {df_discharge.shape}\")\n",
        "print(f\"Radiology notes shape: {df_radiology.shape}\")\n",
        "print()\n",
        "\n",
        "# 2. PREPARE TEXT FEATURES - MEMORY EFFICIENT\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: Preparing Text Features (Memory Efficient)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Combine notes\n",
        "df_notes = pd.concat([df_discharge, df_radiology], ignore_index=True)\n",
        "print(f\"Combined notes shape: {df_notes.shape}\")\n",
        "\n",
        "# Clear memory\n",
        "del df_discharge, df_radiology\n",
        "gc.collect()\n",
        "\n",
        "# Group and aggregate with string limits to reduce memory\n",
        "print(\"Aggregating notes by admission...\")\n",
        "df_notes_agg = df_notes.groupby('hadm_id')['text'].apply(\n",
        "    lambda texts: ' '.join(str(text)[:5000] for text in texts if pd.notna(text))[:20000]  # Limit text length\n",
        ").reset_index()\n",
        "\n",
        "df_notes_agg.columns = ['hadm_id', 'all_notes_text']\n",
        "print(f\"Aggregated notes shape: {df_notes_agg.shape}\")\n",
        "\n",
        "# Clear memory\n",
        "del df_notes\n",
        "gc.collect()\n",
        "print()\n",
        "\n",
        "# 3. PREPARE FINAL DATASET\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 3: Preparing Final Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Merge with aggregated notes\n",
        "df_final = df_admissions.merge(df_notes_agg, on='hadm_id', how='inner')\n",
        "\n",
        "# Clear memory\n",
        "del df_admissions, df_notes_agg\n",
        "gc.collect()\n",
        "\n",
        "# Handle missing text data\n",
        "df_final['all_notes_text'] = df_final['all_notes_text'].fillna('')\n",
        "\n",
        "# Remove rows with empty text or missing target\n",
        "df_final = df_final[df_final['all_notes_text'].str.len() > 50]  # At least 50 chars\n",
        "df_final = df_final.dropna(subset=['readmitted_30day'])\n",
        "\n",
        "print(f\"Final dataset shape: {df_final.shape}\")\n",
        "\n",
        "# SAMPLING FOR MEMORY EFFICIENCY\n",
        "if USE_SAMPLING and len(df_final) > SAMPLE_SIZE:\n",
        "    print(f\"\\n‚ö†Ô∏è  Using stratified sample of {SAMPLE_SIZE} records for memory efficiency\")\n",
        "    df_final = df_final.groupby('readmitted_30day', group_keys=False).apply(\n",
        "        lambda x: x.sample(n=min(len(x), int(SAMPLE_SIZE * (len(x) / len(df_final)))), random_state=42)\n",
        "    ).reset_index(drop=True)\n",
        "    print(f\"Sampled dataset shape: {df_final.shape}\")\n",
        "\n",
        "print(f\"Readmission distribution:\\n{df_final['readmitted_30day'].value_counts()}\")\n",
        "print(f\"Readmission rate: {df_final['readmitted_30day'].mean():.2%}\")\n",
        "print()\n",
        "\n",
        "# 4. DEFINE X AND y\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 4: Defining Features and Target\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "X = df_final['all_notes_text']\n",
        "y = df_final['readmitted_30day'].astype(int)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# Clear memory\n",
        "del df_final\n",
        "gc.collect()\n",
        "print()\n",
        "\n",
        "# 5. TRAIN-TEST SPLIT\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 5: Train-Test Split\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Training readmission rate: {y_train.mean():.2%}\")\n",
        "print(f\"Test readmission rate: {y_test.mean():.2%}\")\n",
        "print()\n",
        "\n",
        "# 6. TF-IDF VECTORIZATION - OPTIMIZED\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 6: TF-IDF Vectorization (Optimized)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer with memory-efficient settings\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    max_features=MAX_FEATURES,\n",
        "    ngram_range=(1, 1),  # Only unigrams to save memory\n",
        "    min_df=MIN_DF,\n",
        "    max_df=MAX_DF,\n",
        "    strip_accents='unicode',\n",
        "    lowercase=True,\n",
        "    dtype=np.float32  # Use float32 instead of float64\n",
        ")\n",
        "\n",
        "# Fit and transform training data\n",
        "print(\"Fitting TF-IDF vectorizer on training data...\")\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "print(\"Transforming test data...\")\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF training matrix shape: {X_train_tfidf.shape}\")\n",
        "print(f\"TF-IDF test matrix shape: {X_test_tfidf.shape}\")\n",
        "\n",
        "# Clear memory\n",
        "del X_train, X_test\n",
        "gc.collect()\n",
        "print()\n",
        "\n",
        "# 7. TRAIN AND EVALUATE ALL MODELS\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 7: Training and Evaluating All Models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate scale_pos_weight for XGBoost\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "# Get feature names for interpretability\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "def evaluate_and_write(model, model_name, X_test, y_test, file_handle, feature_names=None):\n",
        "    \"\"\"\n",
        "    Evaluate a trained model and write results to file AND print to console.\n",
        "\n",
        "    Parameters:\n",
        "    - model: trained sklearn/xgboost model\n",
        "    - model_name: string name of the model\n",
        "    - X_test: test features\n",
        "    - y_test: test labels\n",
        "    - file_handle: open file handle to write to\n",
        "    - feature_names: array of feature names (optional, for feature importance)\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Get probability predictions\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    elif hasattr(model, 'decision_function'):\n",
        "        # For LinearSVC which doesn't have predict_proba by default\n",
        "        y_pred_proba = model.decision_function(X_test)\n",
        "    else:\n",
        "        y_pred_proba = y_pred\n",
        "\n",
        "    # Prepare output content\n",
        "    output = []\n",
        "    output.append(\"=\" * 60)\n",
        "    output.append(f\"MODEL: {model_name}\")\n",
        "    output.append(\"=\" * 60)\n",
        "    output.append(\"\")\n",
        "\n",
        "    # Classification report\n",
        "    output.append(\"üìä CLASSIFICATION REPORT:\")\n",
        "    output.append(\"-\" * 60)\n",
        "    report = classification_report(y_test, y_pred, target_names=['Not Readmitted', 'Readmitted'])\n",
        "    output.append(report)\n",
        "\n",
        "    # Confusion matrix\n",
        "    output.append(\"\\nüìà CONFUSION MATRIX:\")\n",
        "    output.append(\"-\" * 60)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    output.append(str(cm))\n",
        "    output.append(f\"True Negatives:  {cm[0, 0]}\")\n",
        "    output.append(f\"False Positives: {cm[0, 1]}\")\n",
        "    output.append(f\"False Negatives: {cm[1, 0]}\")\n",
        "    output.append(f\"True Positives:  {cm[1, 1]}\")\n",
        "\n",
        "    # ROC-AUC\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    output.append(f\"\\nüéØ ROC-AUC SCORE: {roc_auc:.4f}\")\n",
        "\n",
        "    # Summary metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    output.append(\"\\nüìå SUMMARY METRICS:\")\n",
        "    output.append(\"-\" * 60)\n",
        "    output.append(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    output.append(f\"Precision: {precision:.4f}\")\n",
        "    output.append(f\"Recall:    {recall:.4f}\")\n",
        "    output.append(f\"F1-Score:  {f1:.4f}\")\n",
        "    output.append(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    if feature_names is not None:\n",
        "        output.append(\"\\nüîù TOP 15 MOST PREDICTIVE FEATURES:\")\n",
        "        output.append(\"-\" * 60)\n",
        "\n",
        "        # Check if model has coefficients (Linear models)\n",
        "        if hasattr(model, 'coef_'):\n",
        "            coefficients = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
        "\n",
        "            top_positive_idx = np.argsort(coefficients)[-15:][::-1]\n",
        "            output.append(\"\\nMost predictive of READMISSION:\")\n",
        "            for idx in top_positive_idx:\n",
        "                output.append(f\"  {feature_names[idx]}: {coefficients[idx]:.4f}\")\n",
        "\n",
        "            top_negative_idx = np.argsort(coefficients)[:15]\n",
        "            output.append(\"\\nMost predictive of NO READMISSION:\")\n",
        "            for idx in top_negative_idx:\n",
        "                output.append(f\"  {feature_names[idx]}: {coefficients[idx]:.4f}\")\n",
        "\n",
        "        # Check if model has feature importances (Tree-based models)\n",
        "        elif hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "\n",
        "            top_idx = np.argsort(importances)[-15:][::-1]\n",
        "            output.append(\"\\nTop 15 Most Important Features:\")\n",
        "            for idx in top_idx:\n",
        "                output.append(f\"  {feature_names[idx]}: {importances[idx]:.4f}\")\n",
        "\n",
        "    output.append(\"\")\n",
        "    output.append(\"=\" * 60)\n",
        "    output.append(\"\")\n",
        "\n",
        "    # Join all output lines\n",
        "    full_output = \"\\n\".join(output)\n",
        "\n",
        "    # Write to file\n",
        "    file_handle.write(full_output)\n",
        "    file_handle.flush()  # Ensure it's written immediately\n",
        "\n",
        "    # Print to console\n",
        "    print(full_output)\n",
        "\n",
        "    return accuracy, precision, recall, f1, roc_auc\n",
        "\n",
        "\n",
        "# Initialize output file (create empty file at start)\n",
        "with open('output.txt', 'w') as f:\n",
        "    f.write(\"MODEL EVALUATION RESULTS\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "print(\"\\nüíæ Output file initialized: output.txt\\n\")\n",
        "\n",
        "# Train and evaluate models one by one, writing after each\n",
        "try:\n",
        "    # MODEL 1: Logistic Regression\n",
        "    print(\"üîÑ Training Logistic Regression...\")\n",
        "    logreg = LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        solver='saga',\n",
        "        penalty='l2',\n",
        "        C=1.0,\n",
        "        verbose=0\n",
        "    )\n",
        "    logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    with open('output.txt', 'a') as f:\n",
        "        evaluate_and_write(logreg, \"Logistic Regression\", X_test_tfidf, y_test, f, feature_names)\n",
        "\n",
        "    # Clear model from memory\n",
        "    del logreg\n",
        "    gc.collect()\n",
        "    print(\"‚úÖ Logistic Regression complete and saved!\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error with Logistic Regression: {e}\\n\")\n",
        "\n",
        "try:\n",
        "    # MODEL 2: Linear SVM\n",
        "    print(\"üîÑ Training Linear SVM...\")\n",
        "    linear_svm = LinearSVC(\n",
        "        class_weight='balanced',\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        dual=False,\n",
        "        verbose=0\n",
        "    )\n",
        "    linear_svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    with open('output.txt', 'a') as f:\n",
        "        evaluate_and_write(linear_svm, \"Linear SVM (LinearSVC)\", X_test_tfidf, y_test, f, feature_names)\n",
        "\n",
        "    # Clear model from memory\n",
        "    del linear_svm\n",
        "    gc.collect()\n",
        "    print(\"‚úÖ Linear SVM complete and saved!\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error with Linear SVM: {e}\\n\")\n",
        "\n",
        "try:\n",
        "    # MODEL 3: Random Forest (REDUCED for memory)\n",
        "    print(\"üîÑ Training Random Forest...\")\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=50,  # REDUCED from 100\n",
        "        max_depth=20,  # REDUCED from 25\n",
        "        min_samples_leaf=20,  # INCREASED from 10\n",
        "        max_features='sqrt',  # Add this to reduce memory\n",
        "        class_weight='balanced',\n",
        "        n_jobs=2,  # REDUCED from -1 to limit parallel memory\n",
        "        random_state=42,\n",
        "        verbose=0\n",
        "    )\n",
        "    rf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    with open('output.txt', 'a') as f:\n",
        "        evaluate_and_write(rf, \"Random Forest\", X_test_tfidf, y_test, f, feature_names)\n",
        "\n",
        "    # Clear model from memory\n",
        "    del rf\n",
        "    gc.collect()\n",
        "    print(\"‚úÖ Random Forest complete and saved!\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error with Random Forest: {e}\\n\")\n",
        "\n",
        "try:\n",
        "    # MODEL 4: XGBoost (REDUCED for memory)\n",
        "    print(\"üîÑ Training XGBoost...\")\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=100,  # REDUCED from 200\n",
        "        max_depth=6,  # REDUCED from 8\n",
        "        tree_method='hist',\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,  # Add subsampling to reduce memory\n",
        "        colsample_bytree=0.8,  # Add feature sampling\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        early_stopping_rounds=10,\n",
        "        verbosity=0,\n",
        "        nthread=2  # Limit threads to reduce memory\n",
        "    )\n",
        "    xgb.fit(\n",
        "        X_train_tfidf,\n",
        "        y_train,\n",
        "        eval_set=[(X_test_tfidf, y_test)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    with open('output.txt', 'a') as f:\n",
        "        evaluate_and_write(xgb, \"XGBoost\", X_test_tfidf, y_test, f, feature_names)\n",
        "\n",
        "    # Clear model from memory\n",
        "    del xgb\n",
        "    gc.collect()\n",
        "    print(\"‚úÖ XGBoost complete and saved!\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error with XGBoost: {e}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ ALL MODEL EVALUATIONS COMPLETE!\")\n",
        "print(\"üìÑ Results saved to output.txt\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQEPqdjBkQMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}